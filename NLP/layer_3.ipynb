{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf112ce",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "9421cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "8e182dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\n",
    "    (\"привет\", \"hello\"),\n",
    "    (\"как дела\", \"how are you\"),\n",
    "    (\"спасибо\", \"thank you\"),\n",
    "    (\"пока\", \"bye\"),\n",
    "    (\"доброе утро\", \"good morning\"),\n",
    "    (\"добрый вечер\", \"good evening\"),\n",
    "    (\"я люблю тебя\", \"i love you\"),\n",
    "    (\"что это\", \"what is this\"),\n",
    "    (\"где ты\", \"where are you\"),\n",
    "    (\"хорошо\", \"ok\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "af05bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocabs(sentences, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for s in sentences:\n",
    "        counter.update(s.split())\n",
    "    word2idx = {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    idx2word = {0: '<pad>', 1: '<bos>', 2: '<eos>', 3: '<unk>'}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq and word not in word2idx:\n",
    "            idx = len(word2idx)\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "src_sentences = [s for s, _ in toy_corpus]\n",
    "tr_sentences = [s for _, s in toy_corpus]\n",
    "src_w2i, src_i2w = build_vocabs(src_sentences)\n",
    "tr_w2i, tr_i2w = build_vocabs(tr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "6e8d406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence, w2i, add_bos=False, add_eos=False):\n",
    "    tokens = []\n",
    "    if add_bos: tokens.append(w2i['<bos>'])\n",
    "    \n",
    "    for w in sentence.split():\n",
    "        tokens.append(w2i.get(w, w2i['<unk>']))        \n",
    "    \n",
    "    if add_eos: tokens.append(w2i['<eos>'])\n",
    "    return tokens\n",
    "\n",
    "def pad_batch(seqs, pad_idx):\n",
    "    max_len = max(len(s) for s in seqs)\n",
    "    return [s + [pad_idx] * (max_len-len(s)) for s in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "f0256634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyCorpusDataset(Dataset):\n",
    "    def __init__(self, corpus, src_w2i, tr_w2i):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus\n",
    "        self.src_vocab = src_w2i\n",
    "        self.tr_vocab = tr_w2i\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "    def __getitem__(self, index):\n",
    "        src, tr = self.corpus[index]\n",
    "        src_ids = encode(src, self.src_vocab)\n",
    "        tr_ids = encode(tr, self.tr_vocab, True, True)\n",
    "\n",
    "        return torch.tensor(src_ids), torch.tensor(tr_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tr_batch = zip(*batch)\n",
    "    src_lens = torch.tensor([len(s) for s in src_batch])\n",
    "    tr_lens = torch.tensor([len(s) for s in tr_batch])\n",
    "    src_padded = torch.tensor(pad_batch([s.tolist() for s in src_batch], src_w2i['<pad>']))\n",
    "    tr_padded = torch.tensor(pad_batch([s.tolist() for s in tr_batch], tr_w2i['<pad>']))\n",
    "\n",
    "    return src_padded, src_lens, tr_padded, tr_lens\n",
    "\n",
    "dataset = ToyCorpusDataset(toy_corpus, src_w2i, tr_w2i)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "68580766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, vocab_size, pad_idx, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)    \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_idx)        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)                \n",
    "        \n",
    "    def forward(self, x, lens):\n",
    "        embed = self.embedding(x)       \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embed, lengths=lens, batch_first=True, enforce_sorted=False)         \n",
    "        packed_out, hidden = self.gru(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "dc5a46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_layers, vocab_size, pad_idx, *args, **kwargs):        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_idx)        \n",
    "        self.attn_W1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attn_W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attn_v = nn.Linear(hidden_size, 1, False)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_dim + hidden_size, hidden_size, num_layers, batch_first=True)        \n",
    "        self.out = nn.Linear(hidden_size * 2, vocab_size)\n",
    "    def forward(self, x, hidden, enc_out, src_lens=None):        \n",
    "        # x: (B, 1)\n",
    "        embed = self.embedding(x) # (b, 1, e)       \n",
    "\n",
    "        hidden_exp = hidden[-1].unsqueeze(1).expand(-1, enc_out.size(1), -1)\n",
    "        score = self.attn_v(torch.tanh(self.attn_W1(hidden_exp) + self.attn_W2(enc_out))).squeeze(2)\n",
    "        if src_lens is not None: #masking paddings \n",
    "            mask = torch.arange(enc_out.size(1), device=enc_out.device).unsqueeze(0) >= src_lens.unsqueeze(1)\n",
    "            score.masked_fill_(mask, float('-inf'))\n",
    "        attn_weights = torch.softmax(score, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), enc_out)\n",
    "\n",
    "        gru_input = torch.cat([embed, context], dim=2)                \n",
    "        output, hidden = self.gru(gru_input, hidden) # (b, 1, h), (num_layers, b, h)                                \n",
    "        output = torch.cat([output, context], dim=2)   \n",
    "        logits = self.out(output.squeeze(1)) \n",
    "        return logits, hidden    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "8a8712fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, bos_idx, eos_idx, pad_idx, teacher_forcing_ratio=1., *args, **kwargs):        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio        \n",
    "\n",
    "    def forward(self, src,src_lens, tr=None, max_len=20):\n",
    "        batch_size = src.size(0)\n",
    "        enc_out, h = self.encoder(src, src_lens) # enc_out for attention in future             \n",
    "        teacher_forcing_ratio = self.teacher_forcing_ratio\n",
    "\n",
    "        outputs = []\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool, device=src.device)\n",
    "\n",
    "        y_prev = torch.full((batch_size,  1), self.bos_idx, dtype=torch.long, device=src.device)        \n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            logits, h = self.decoder(y_prev, h, enc_out, src_lens)\n",
    "            outputs.append(logits.unsqueeze(1))                    \n",
    "\n",
    "            if self.training and tr is not None:\n",
    "                teacher_force =  torch.rand(1).item() < teacher_forcing_ratio\n",
    "                y_prev = tr[:, t].unsqueeze(1) if teacher_force and t < tr.size(1) else logits.argmax(-1).unsqueeze(1)                \n",
    "            else:\n",
    "                y_prev = logits.argmax(-1).unsqueeze(1)\n",
    "                        \n",
    "            finished |= (y_prev.squeeze(1) == self.eos_idx)\n",
    "            if finished.all():\n",
    "                break        \n",
    "            \n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def translate(self, src, src_lens, max_len=20):\n",
    "        self.eval()\n",
    "        batch_size = src.size(0)\n",
    "        enc_out, h = self.encoder(src, src_lens)\n",
    "\n",
    "        y_prev = torch.full((batch_size, 1), self.bos_idx, dtype=torch.long, device=src.device)\n",
    "        outputs = []\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool, device=src.device)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            logits, h = self.decoder(y_prev, h, enc_out, src_lens)\n",
    "            pred = logits.argmax(-1).unsqueeze(1)\n",
    "            outputs.append(pred)\n",
    "\n",
    "            y_prev = pred\n",
    "            finished |= (pred.squeeze(1) == self.eos_idx)\n",
    "            if finished.all():\n",
    "                break\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "f9a66cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(embedding_dim=8, hidden_size=8, num_layers=2, vocab_size=len(src_w2i), pad_idx=src_w2i['<pad>'])\n",
    "decoder = Decoder(embedding_dim=8, hidden_size=8, num_layers=2, vocab_size=len(tr_w2i), pad_idx=tr_w2i['<pad>'])\n",
    "seq2seq = Seq2Seq(encoder, decoder, bos_idx=tr_w2i['<bos>'], eos_idx=tr_w2i['<eos>'], pad_idx=tr_w2i['<pad>'], teacher_forcing_ratio=0.8)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tr_w2i['<pad>'])\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "f9dddccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Loss: 2.0845\n",
      "Epoch 100. Loss: 1.2566\n",
      "Epoch 150. Loss: 0.8559\n",
      "Epoch 200. Loss: 0.5890\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "seq2seq.to(device)\n",
    "best_bleu = 0.0\n",
    "early_stopping_rounds, early_stopping_rounds_counter = 1000, 0\n",
    "\n",
    "for epoch in range(1, 201):    \n",
    "    seq2seq.train()\n",
    "    total_loss = 0        \n",
    "    for src, src_lens, tr, tr_lens in dataloader:\n",
    "        src, tr = src.to(device), tr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = seq2seq(src, src_lens, tr, max_len=tr.size(1))        \n",
    "        \n",
    "        loss = loss_fn(\n",
    "            logits.reshape(-1, logits.size(2)),\n",
    "            tr[:, :logits.size(1)].reshape(-1),\n",
    "        )\n",
    "        \n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    seq2seq.teacher_forcing_ratio *= 0.99\n",
    "\n",
    "    # seq2seq.eval()\n",
    "    # candidate_corpus, references_corpus = [], []\n",
    "    # with torch.no_grad():\n",
    "    #     for src, src_lens, tr, tr_lens in dataloader:\n",
    "    #         src, tr = src.to(device), tr.to(device)\n",
    "    #         preds = seq2seq.translate(src, src_lens, max_len=tr.size(1))                        \n",
    "\n",
    "    #         for i in range(len(src)):\n",
    "    #             hyp = [tr_i2w[idx.item()] for idx in preds[i]\n",
    "    #                    if idx.item() not in  [tr_w2i['<pad>'], tr_w2i['<bos>'], tr_w2i['<eos>']]\n",
    "    #             ]\n",
    "    #             ref = [[tr_i2w[idx.item()] for idx in tr[i] \n",
    "    #                     if idx.item() not in [tr_w2i['<pad>'], tr_w2i['<bos>'], tr_w2i['<eos>']]]]\n",
    "                \n",
    "    #             candidate_corpus.append(hyp)\n",
    "    #             references_corpus.append(ref[0])            \n",
    "    # bleu = bleu_score(candidate_corpus, references_corpus)\n",
    "\n",
    "    # if bleu > best_bleu:\n",
    "    #     best_bleu = bleu\n",
    "    #     early_stopping_rounds_counter = 0\n",
    "    # else:\n",
    "    #     early_stopping_rounds_counter += 1\n",
    "    #     if early_stopping_rounds_counter >= early_stopping_rounds:\n",
    "    #         print(f'Early stopping on epoch {epoch}, best BLEU {best_bleu:.4f}')\n",
    "    #         break    \n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        # print(f'Epoch {epoch}. Loss: {total_loss/len(dataloader):.4f}, Bleu: {bleu:.4f}')\n",
    "        print(f'Epoch {epoch}. Loss: {total_loss/len(dataloader):.4f}')\n",
    "        # for i in range(2):            \n",
    "        #     print('ref:', references_corpus[-i])        \n",
    "        #     print('hyp:', candidate_corpus[-i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "989f40e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: привет как дела\n",
      "Pred : where you you you you you you you you\n",
      "\n",
      "Source: где ты я люблю тебя\n",
      "Pred : i love you you you you you you you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_src = [\"привет как дела\", \"где ты я люблю тебя\"]\n",
    "\n",
    "src_ids = [torch.tensor(encode(s, src_w2i)) for s in test_src]\n",
    "src_padded = torch.tensor(pad_batch([s.tolist() for s in src_ids], src_w2i['<pad>']))\n",
    "src_lens = torch.tensor([len(s) for s in src_ids])\n",
    "\n",
    "src_padded = src_padded.to(device)\n",
    "src_lens = src_lens.to(device)\n",
    "\n",
    "seq2seq.eval()\n",
    "with torch.no_grad():\n",
    "    preds = seq2seq.translate(src_padded, src_lens, max_len=10)\n",
    "\n",
    "for i, pred in enumerate(preds):\n",
    "    words = [tr_i2w[idx.item()] for idx in pred if idx.item() not in [tr_w2i['<pad>'], tr_w2i['<bos>'], tr_w2i['<eos>']]]\n",
    "    print(f\"Source: {test_src[i]}\")\n",
    "    print(f\"Pred : {' '.join(words)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf983a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

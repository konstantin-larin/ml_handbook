{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c9ea8f",
   "metadata": {},
   "source": [
    "Implementation of linear binary classification models from scratch and their comparison. Extension to multiclass logistic regression using the softmax function, along with a comparison of softmax to other multiclass strategies such as One-vs-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d761176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cc86e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 344 entries, 0 to 343\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   species            344 non-null    object \n",
      " 1   island             344 non-null    object \n",
      " 2   bill_length_mm     342 non-null    float64\n",
      " 3   bill_depth_mm      342 non-null    float64\n",
      " 4   flipper_length_mm  342 non-null    float64\n",
      " 5   body_mass_g        342 non-null    float64\n",
      " 6   sex                333 non-null    object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 18.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    Male  \n",
       "1       3800.0  Female  \n",
       "2       3250.0  Female  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  Female  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = sns.load_dataset('penguins')\n",
    "\n",
    "\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d881fce",
   "metadata": {},
   "source": [
    "Binary classification – predicting whether the species of a penguin is \"name of species\" or not (1 or 0).\n",
    "Multiclass classification – predicting the species of a penguin using softmax or by applying multiple binary classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9cb4a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male' 'Female' nan]\n",
      "['Adelie' 'Chinstrap' 'Gentoo']\n",
      "['Torgersen' 'Biscoe' 'Dream']\n"
     ]
    }
   ],
   "source": [
    "print(df['sex'].unique())\n",
    "print(df['species'].unique())\n",
    "print(df['island'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832a85e",
   "metadata": {},
   "source": [
    "Let's drop Nans from sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321657f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male' 'Female']\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['sex'], inplace=True)\n",
    "print(df['sex'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe180e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species              0\n",
       "island               0\n",
       "bill_length_mm       0\n",
       "bill_depth_mm        0\n",
       "flipper_length_mm    0\n",
       "body_mass_g          0\n",
       "sex                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cefb830",
   "metadata": {},
   "source": [
    "By removing nans from sex we removed other missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b554b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "numerical_features = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "categorical_features = ['island', 'sex']\n",
    "classes_dict = {\n",
    "    'Adelie': 0,\n",
    "    'Chinstrap': 1, \n",
    "    'Gentoo': 2,\n",
    "}\n",
    "X = df.drop(columns=['species'])\n",
    "y = df['species'].map(classes_dict)\n",
    "print(y.unique())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=0.8, random_state=42)\n",
    "ss = StandardScaler()\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ss.fit(X_train[numerical_features])\n",
    "ohe.fit(X_train[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab25686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    X_numerical = X[numerical_features]\n",
    "    X_numerical = ss.transform(X_numerical)\n",
    "    X_categorical = X[categorical_features]\n",
    "    X_categorical = ohe.transform(X_categorical)\n",
    "    return np.concatenate((X_numerical, X_categorical), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4563a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = preprocess_data(X_train), preprocess_data(X_test)\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008d72f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_0_train, y_0_test = (y_train == 0).astype('int'), (y_test == 0).astype('int')\n",
    "y_1_train, y_1_test = (y_train == 1).astype('int'), (y_test == 1).astype('int')\n",
    "y_2_train, y_2_test = (y_train == 2).astype('int'), (y_test == 2).astype('int')\n",
    "y_0_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024c4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, lambda_reg=1e-2):\n",
    "        self.alpha = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_reg = lambda_reg / 2 #for convenience \n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def _cross_entropy(self, y, p):           \n",
    "        return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p)) + self.lambda_reg * np.sum(self.w ** 2)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    def fit(self, X, y, border=0.0001):\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.random.randn(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        old_loss = float('inf')\n",
    "        for epoch in range(self.epochs):\n",
    "            p = self.predict_proba(X)\n",
    "\n",
    "            loss = self._cross_entropy(y, p)\n",
    "            if epoch % 100 == 0:\n",
    "                print('Cross entropy:', loss)\n",
    "            if abs(old_loss - loss) < border:\n",
    "                break\n",
    "            old_loss = loss\n",
    "                  \n",
    "            error = y - p\n",
    "            grad = -X.T.dot(error)\n",
    "            reg = self.lambda_reg * self.w # L2\n",
    "            self.w -= self.alpha * (grad + reg)\n",
    "            self.b -= self.alpha * np.sum(error)\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.w) + self.b\n",
    "        p = np.clip(self._sigmoid(z), 1e-10, 1 - 1e-10)\n",
    "        return p\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        p = self.predict_proba(X)\n",
    "        return np.where(p > threshold, 1, 0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2b1361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 217.42848681929758\n",
      "Cross entropy: 5.687112063408204\n",
      "Cross entropy: 3.97384561042776\n",
      "Cross entropy: 3.3423527428912694\n",
      "Cross entropy: 3.01935404714619\n",
      "Cross entropy: 2.828287222923561\n",
      "Cross entropy: 2.706079742437638\n",
      "Cross entropy: 2.6244813144402532\n",
      "Cross entropy: 2.5689259339617427\n",
      "Cross entropy: 2.531137697370192\n",
      "size of y_0 train (266,) test (67,)\n",
      "f1 of logreg, predict adelie: 1.0\n",
      "accuracy of logreg, predict adelie 1.0\n",
      "\n",
      "\n",
      "Cross entropy: 314.7032493020353\n",
      "Cross entropy: 2219.890638473168\n",
      "Cross entropy: 3184.479421382969\n",
      "Cross entropy: 3945.705229417273\n",
      "Cross entropy: 4786.057261210821\n",
      "Cross entropy: 5778.2691464471045\n",
      "Cross entropy: 6950.414079523363\n",
      "Cross entropy: 8318.988673850192\n",
      "Cross entropy: 9887.429254072686\n",
      "Cross entropy: 11663.547360652901\n",
      "size of y_1 train (266,) test (67,)\n",
      "f1 of logreg, predict chinstrap: 0.39436619718309857\n",
      "accuracy of logreg, predict chinstrap 0.3582089552238806\n",
      "\n",
      "\n",
      "Cross entropy: 685.4737020626519\n",
      "Cross entropy: 9.112268162506211\n",
      "Cross entropy: 10.239756593540735\n",
      "Cross entropy: 12.908901333979628\n",
      "Cross entropy: 16.754205292620448\n",
      "Cross entropy: 21.799234834083073\n",
      "Cross entropy: 28.109351888426666\n",
      "Cross entropy: 35.746290968557645\n",
      "Cross entropy: 44.764758224171594\n",
      "Cross entropy: 55.21606391686534\n",
      "size of y_2 train (266,) test (67,)\n",
      "f1 of logreg, predict gentoo: 0.9411764705882353\n",
      "accuracy of logreg, predict gentoo 0.9552238805970149\n"
     ]
    }
   ],
   "source": [
    "# test classificators on one class\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "blogreg = BinaryLogisticRegression()\n",
    "\n",
    "blogreg.fit(X_train, y_0_train)\n",
    "print('size of y_0 train', y_0_train.shape, 'test', y_0_test.shape)\n",
    "print(\"f1 of logreg, predict adelie:\", f1_score(blogreg.predict(X_test), y_0_test))\n",
    "print('accuracy of logreg, predict adelie', accuracy_score(blogreg.predict(X_test), y_0_test))\n",
    "\n",
    "print('\\n')\n",
    "blogreg.fit(X_train, y_1_train)\n",
    "print('size of y_1 train', y_1_train.shape, 'test', y_1_test.shape)\n",
    "print(\"f1 of logreg, predict chinstrap:\", f1_score(blogreg.predict(X_test), y_1_test))\n",
    "print('accuracy of logreg, predict chinstrap', accuracy_score(blogreg.predict(X_test), y_1_test))\n",
    "\n",
    "print('\\n')\n",
    "blogreg.fit(X_train, y_2_train)\n",
    "print('size of y_2 train', y_2_train.shape, 'test', y_2_test.shape)\n",
    "print(\"f1 of logreg, predict gentoo:\", f1_score(blogreg.predict(X_test), y_2_test))\n",
    "print('accuracy of logreg, predict gentoo', accuracy_score(blogreg.predict(X_test), y_2_test))\n",
    "\n",
    "# bsvm.fit(X_train, y_0_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be6268",
   "metadata": {},
   "source": [
    "Binary Log regressor clearly recognizes classes 0 and 2 but has big troubles with class 1\n",
    "maybe deal is in learning_rate? (Bloom of gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59f0f3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 126.51651332296979\n",
      "Cross entropy: 126.44189697134425\n",
      "Cross entropy: 126.36917737366221\n",
      "Cross entropy: 126.29831596611905\n",
      "Cross entropy: 126.22927507753165\n",
      "Cross entropy: 126.16201791067378\n",
      "Cross entropy: 126.0965085238647\n",
      "Cross entropy: 126.032711812813\n",
      "Cross entropy: 125.97059349272128\n",
      "Cross entropy: 125.91012008065348\n",
      "size of y_1 train (266,) test (67,)\n",
      "f1 of logreg, predict chinstrap: 0.717948717948718\n",
      "accuracy of logreg, predict chinstrap 0.835820895522388\n"
     ]
    }
   ],
   "source": [
    "blogreg = BinaryLogisticRegression(learning_rate=0.000001)\n",
    "\n",
    "blogreg.fit(X_train, y_1_train)\n",
    "print('size of y_1 train', y_1_train.shape, 'test', y_1_test.shape)\n",
    "print(\"f1 of logreg, predict chinstrap:\", f1_score(blogreg.predict(X_test), y_1_test))\n",
    "print('accuracy of logreg, predict chinstrap', accuracy_score(blogreg.predict(X_test), y_1_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016ede4",
   "metadata": {},
   "source": [
    "not enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b1e67c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'test classes distr')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "y_labels, y_unique_counts = np.unique(y_train, return_counts=True)\n",
    "axs[0].bar(y_labels, y_unique_counts)\n",
    "axs[0].set_title('train classes distr')\n",
    "\n",
    "y_labels, y_unique_counts = np.unique(y_test, return_counts=True)\n",
    "axs[1].bar(y_labels, y_unique_counts)\n",
    "axs[1].set_title('test classes distr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6430c8",
   "metadata": {},
   "source": [
    "test and train are stratified - OK. But class \"Chilstrap\" is minor.\n",
    "Problem is disbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3568709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89261745 1.13675214]\n",
      "[0.62735849 2.46296296]\n",
      "[0.77777778 1.4       ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_0_weigths = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y = y_0_train,\n",
    ")\n",
    "print(y_0_weigths)\n",
    "\n",
    "y_1_weigths = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y = y_1_train,\n",
    ")\n",
    "print(y_1_weigths)\n",
    "\n",
    "y_2_weigths = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y = y_2_train,\n",
    ")\n",
    "print(y_2_weigths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6a53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, lambda_reg=1e-2):\n",
    "        self.alpha = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_reg = lambda_reg / 2 #for convenience \n",
    "        self.w = None\n",
    "        self.class_weights = None\n",
    "        self.b = None        \n",
    "        \n",
    "    def _cross_entropy(self, y, p):\n",
    "        loss = y * np.log(p) + (1 - y) * np.log(1 - p)\n",
    "        if self.class_weights is not None:\n",
    "            sample_weights = np.array([self.class_weights[int(cl)] for cl in y])            \n",
    "            # print(sample_weights)\n",
    "            loss = loss * sample_weights\n",
    "        return -np.sum(loss) + self.lambda_reg * np.sum(self.w ** 2)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    def fit(self, X, y, border=0.0001, class_weights=None):        \n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.random.randn(n_features) * 0.01\n",
    "        self.b = 0\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "        old_loss = float('inf')\n",
    "        for epoch in range(self.epochs):\n",
    "            p = self.predict_proba(X)\n",
    "\n",
    "            loss = self._cross_entropy(y, p)\n",
    "            if epoch % 100 == 0:\n",
    "                print('Cross entropy:', loss)\n",
    "            if abs(old_loss - loss) < border:\n",
    "                break\n",
    "            old_loss = loss\n",
    "                  \n",
    "            error = y - p\n",
    "            if self.class_weights is not None:\n",
    "                sample_weights = np.array([self.class_weights[int(cl)] for cl in y])                \n",
    "                error = error * sample_weights\n",
    "            grad = -X.T.dot(error)\n",
    "            reg = self.lambda_reg * self.w # L2\n",
    "            self.w -= self.alpha * (grad + reg)\n",
    "            self.b -= self.alpha * np.sum(error)\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.w) + self.b\n",
    "        p = np.clip(self._sigmoid(z), 1e-10, 1 - 1e-10)\n",
    "        return p\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        p = self.predict_proba(X)\n",
    "        return np.where(p > threshold, 1, 0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "874c56e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 186.74308083104486\n",
      "Cross entropy: 8.645818403510685\n",
      "Cross entropy: 7.876001466652976\n",
      "size of y_0 train (266,) test (67,)\n",
      "f1 of logreg, predict adelie: 1.0\n",
      "accuracy of logreg, predict adelie 1.0\n",
      "\n",
      "\n",
      "Cross entropy: 190.1733084255526\n",
      "Cross entropy: 5.024698802195819\n",
      "size of y_2 train (266,) test (67,)\n",
      "f1 of logreg, predict gentoo: 0.9795918367346939\n",
      "accuracy of logreg, predict gentoo 0.9850746268656716\n",
      "\n",
      "\n",
      "Cross entropy: 184.30878777142706\n",
      "Cross entropy: 111.46792048299169\n",
      "Cross entropy: 92.8706947436014\n",
      "Cross entropy: 84.91565064568093\n",
      "Cross entropy: 81.02048597314712\n",
      "Cross entropy: 79.21842943407195\n",
      "Cross entropy: 78.6902320898044\n",
      "size of y_1 train (266,) test (67,)\n",
      "f1 of logreg, predict chinstrap: 0.7368421052631579\n",
      "accuracy of logreg, predict chinstrap 0.8507462686567164\n"
     ]
    }
   ],
   "source": [
    "# test classificators on one class\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "blogreg = BinaryLogisticRegression()\n",
    "\n",
    "blogreg.fit(X_train, y_0_train) # it's ideal - don't touch\n",
    "print('size of y_0 train', y_0_train.shape, 'test', y_0_test.shape)\n",
    "print(\"f1 of logreg, predict adelie:\", f1_score(blogreg.predict(X_test), y_0_test))\n",
    "print('accuracy of logreg, predict adelie', accuracy_score(blogreg.predict(X_test), y_0_test))\n",
    "\n",
    "print('\\n')\n",
    "blogreg.fit(X_train, y_2_train, class_weights=y_2_weigths)\n",
    "print('size of y_2 train', y_2_train.shape, 'test', y_2_test.shape)\n",
    "print(\"f1 of logreg, predict gentoo:\", f1_score(blogreg.predict(X_test), y_2_test))\n",
    "print('accuracy of logreg, predict gentoo', accuracy_score(blogreg.predict(X_test), y_2_test))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "blogreg = BinaryLogisticRegression(learning_rate=0.0001)\n",
    "blogreg.fit(X_train, y_1_train, class_weights=y_1_weigths)\n",
    "print('size of y_1 train', y_1_train.shape, 'test', y_1_test.shape)\n",
    "print(\"f1 of logreg, predict chinstrap:\", f1_score(blogreg.predict(X_test), y_1_test))\n",
    "print('accuracy of logreg, predict chinstrap', accuracy_score(blogreg.predict(X_test), y_1_test))\n",
    "\n",
    "\n",
    "# bsvm.fit(X_train, y_0_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b323182",
   "metadata": {},
   "source": [
    "It's better but also we can select optmial threshold (because results are medicore) by best f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e739bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 183.64340573106384\n",
      "Cross entropy: 111.36643085360538\n",
      "Cross entropy: 92.77947003824387\n",
      "Cross entropy: 84.78768608706876\n",
      "Cross entropy: 80.8558357212344\n",
      "Cross entropy: 79.02147725590595\n",
      "Cross entropy: 78.46437016372218\n",
      "f1 0.9333333333333333 threshold 0.7\n"
     ]
    }
   ],
   "source": [
    "best_f1_score = 0\n",
    "best_threshold = 0.5\n",
    "blogreg = BinaryLogisticRegression(learning_rate=0.0001)\n",
    "blogreg.fit(X_train, y_1_train, class_weights=y_1_weigths)\n",
    "for i in range(10, 90, 5):\n",
    "    thr = i / 100    \n",
    "    f1 = f1_score(blogreg.predict(X_test, threshold=thr), y_1_test)\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_threshold = thr    \n",
    "print(\"f1\", best_f1_score, \"threshold\", best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a06df",
   "metadata": {},
   "source": [
    "Nice! It's acceptable. Too much False Positives - it was problem. \n",
    "Sure, in a real case, the threshold should be selected based on the validation set, not the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "064601bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMBinaryClassificator:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, lambda_reg=1e-2):\n",
    "        self.alpha = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_reg = lambda_reg / 2 #for convenience \n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def _hinge_loss(self, margin):        \n",
    "        return np.sum(np.maximum(0, 1 - margin)) + self.lambda_reg * np.sum(self.w ** 2)\n",
    "    \n",
    "            \n",
    "\n",
    "    def _calc_margin(self, X,y):        \n",
    "        return y * (X.dot(self.w) + self.b) \n",
    "    \n",
    "    def fit(self, X, y, border=0.0001):\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.random.randn(n_features)\n",
    "        self.b = 0\n",
    "        y_relabeled = y.copy()\n",
    "        y_relabeled[y_relabeled == 0] = -1\n",
    "        \n",
    "\n",
    "        old_loss = float('inf')\n",
    "        for epoch in range(self.epochs):            \n",
    "            margin = self._calc_margin(X, y_relabeled)\n",
    "            loss = self._hinge_loss(margin)\n",
    "            if epoch % 100 == 0:\n",
    "                print('Hinge loss:', loss)\n",
    "            if abs(old_loss - loss) < border:\n",
    "                break\n",
    "            old_loss = loss            \n",
    "            \n",
    "            is_false = 1 > margin\n",
    "            grad_base = np.where(is_false, -y_relabeled, 0)            \n",
    "\n",
    "            self.w -= self.alpha * (X.T.dot(grad_base) + self.lambda_reg * self.w)\n",
    "            self.b -= self.alpha * np.sum(grad_base)              \n",
    "        return self \n",
    "\n",
    "    def predict_margin(self, X):\n",
    "        return X.dot(self.w) + self.b\n",
    "    def predict(self, X):\n",
    "        y = np.sign(self.predict_margin(X))            \n",
    "        y[y == -1] = 0\n",
    "        return y                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bd321c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinge loss: 47.2401115482427\n",
      "Hinge loss: 2.2829351130901343\n",
      "Hinge loss: 1.7919300768400754\n",
      "size of y_0 train (266,) test (67,)\n",
      "f1 of logreg, predict adelie: 1.0\n",
      "accuracy of logreg, predict adelie 1.0\n",
      "\n",
      "\n",
      "Hinge loss: 344.20423444404105\n",
      "Hinge loss: 2.8368301960065803\n",
      "Hinge loss: 2.0358779485172906\n",
      "Hinge loss: 1.716884427181188\n",
      "Hinge loss: 1.56272573640023\n",
      "size of y_1 train (266,) test (67,)\n",
      "f1 of logreg, predict chinstrap: 0.9655172413793104\n",
      "accuracy of logreg, predict chinstrap 0.9850746268656716\n",
      "\n",
      "\n",
      "Hinge loss: 489.99038447701616\n",
      "size of y_2 train (266,) test (67,)\n",
      "f1 of logreg, predict gentoo: 1.0\n",
      "accuracy of logreg, predict gentoo 1.0\n"
     ]
    }
   ],
   "source": [
    "# test classificators on one class\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "svm_classifier = SVMBinaryClassificator()\n",
    "\n",
    "svm_classifier.fit(X_train, y_0_train)\n",
    "\n",
    "\n",
    "print('size of y_0 train', y_0_train.shape, 'test', y_0_test.shape)\n",
    "print(\"f1 of logreg, predict adelie:\", f1_score(svm_classifier.predict(X_test), y_0_test))\n",
    "print('accuracy of logreg, predict adelie', accuracy_score(svm_classifier.predict(X_test), y_0_test))\n",
    "\n",
    "print('\\n')\n",
    "svm_classifier.fit(X_train, y_1_train)\n",
    "print('size of y_1 train', y_1_train.shape, 'test', y_1_test.shape)\n",
    "print(\"f1 of logreg, predict chinstrap:\", f1_score(svm_classifier.predict(X_test), y_1_test))\n",
    "print('accuracy of logreg, predict chinstrap', accuracy_score(svm_classifier.predict(X_test), y_1_test))\n",
    "\n",
    "print('\\n')\n",
    "svm_classifier.fit(X_train, y_2_train)\n",
    "print('size of y_2 train', y_2_train.shape, 'test', y_2_test.shape)\n",
    "print(\"f1 of logreg, predict gentoo:\", f1_score(svm_classifier.predict(X_test), y_2_test))\n",
    "print('accuracy of logreg, predict gentoo', accuracy_score(svm_classifier.predict(X_test), y_2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09849b7",
   "metadata": {},
   "source": [
    "Great! \n",
    "As we can see, the SVM handled class recognition without balancing, unlike logistic regression.\n",
    "\n",
    "Important observation!\n",
    "SVMs are inherently more robust to class imbalance due to their loss function (Hinge Loss), which ignores most correctly classified majority examples. They naturally focus on examples that are closest to the decision boundary (support vectors), which often include instances from the minority class.\n",
    "\n",
    "Logistic regression, on the other hand, attempts to minimize the error across all examples. A large number of majority class samples can \"outweigh\" the influence of minority ones, leading to a shifted decision boundary and poor performance on the minority class unless additional measures are taken (such as class weighting or oversampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d649a66",
   "metadata": {},
   "source": [
    "### let's realize multiclassification. One versus All for both models and softmax for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a3f5285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy: 183.04279679933958\n",
      "Cross entropy: 8.538529141506253\n",
      "Cross entropy: 7.7592061339100065\n",
      "Cross entropy: 183.51775039882514\n",
      "Cross entropy: 110.6206894119458\n",
      "Cross entropy: 91.9363287486012\n",
      "Cross entropy: 83.9095991716592\n",
      "Cross entropy: 79.9464676023085\n",
      "Cross entropy: 78.07542180609622\n",
      "Cross entropy: 77.4752819352035\n",
      "Cross entropy: 187.11274247024153\n",
      "Cross entropy: 5.030672594731253\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "\n",
      "Logistic Regression One-Versus-All results\n",
      "f1: 0.9826577939100626\n",
      "accuracy: 0.9850746268656716\n"
     ]
    }
   ],
   "source": [
    "classifier_0 = BinaryLogisticRegression()\n",
    "classifier_1 = BinaryLogisticRegression(learning_rate=0.0001)\n",
    "classifier_2 = BinaryLogisticRegression()\n",
    "\n",
    "classifier_0.fit(X_train, y_0_train)\n",
    "classifier_1.fit(X_train, y_1_train, class_weights=y_1_weigths)\n",
    "classifier_2.fit(X_train, y_2_train, class_weights=y_2_weigths)\n",
    "\n",
    "proba_0 = classifier_0.predict_proba(X_test)\n",
    "proba_1 = classifier_1.predict_proba(X_test)\n",
    "proba_2 = classifier_2.predict_proba(X_test)\n",
    "\n",
    "y_pred = np.argmax(np.stack([proba_0, proba_1, proba_2], axis=1), axis=1)\n",
    "print(np.unique(y_pred))\n",
    "print(np.unique(y_test))\n",
    "\n",
    "print('\\nLogistic Regression One-Versus-All results')\n",
    "print('f1:', f1_score(y_test, y_pred, average='macro'))\n",
    "print('accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2b2f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinge loss: 710.263576043462\n",
      "Hinge loss: 241.20242302867416\n",
      "Hinge loss: 3.328370910888577\n",
      "Hinge loss: 2.3095236229667435\n",
      "Hinge loss: 1.8597967956793395\n",
      "Hinge loss: 1.6354550755047133\n",
      "Hinge loss: 1.514642894127963\n",
      "Hinge loss: 1.4758519076983938\n",
      "Hinge loss: 1.407374211096808\n",
      "Hinge loss: 1.3073953911900498\n",
      "Hinge loss: 1.2584583078424454\n",
      "Hinge loss: 29.47466972529027\n",
      "[0 1 2]\n",
      "[0 1 2]\n",
      "\n",
      "SVM One-Versus-All results\n",
      "f1: 0.9826577939100626\n",
      "accuracy: 0.9850746268656716\n"
     ]
    }
   ],
   "source": [
    "classifier_0 = SVMBinaryClassificator()\n",
    "classifier_1 = SVMBinaryClassificator()\n",
    "classifier_2 = SVMBinaryClassificator()\n",
    "\n",
    "classifier_0.fit(X_train, y_0_train)\n",
    "classifier_1.fit(X_train, y_1_train,)\n",
    "classifier_2.fit(X_train, y_2_train,)\n",
    "\n",
    "proba_0 = classifier_0.predict_margin(X_test)\n",
    "proba_1 = classifier_1.predict_margin(X_test)\n",
    "proba_2 = classifier_2.predict_margin(X_test)\n",
    "\n",
    "y_pred = np.argmax(np.stack([proba_0, proba_1, proba_2], axis=1), axis=1)\n",
    "print(np.unique(y_pred))\n",
    "print(np.unique(y_test))\n",
    "\n",
    "print('\\nSVM One-Versus-All results')\n",
    "print('f1:', f1_score(y_test, y_pred, average='macro'))\n",
    "print('accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06199e",
   "metadata": {},
   "source": [
    "Both of them are good\n",
    "Let's realize Universal Logistic Regression with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dda52b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, lambda_reg=1e-2):\n",
    "        self.alpha = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_reg = lambda_reg / 2 #for convenience \n",
    "        self.W = None\n",
    "        self.class_weights = None\n",
    "        self.b = None        \n",
    "    \n",
    "    def _softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1,keepdims=True)) # для числовой стабильности, чтоб не возводить экспоненту в большую степень\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)#векторно вычисляем каждую вероятность\n",
    "    def _cross_entropy(self, y, y_onehot, probabilites):\n",
    "        error = y_onehot * np.log(probabilites)\n",
    "        if self.class_weights is not None:\n",
    "            sample_weights = [self.class_weights[cl] for cl in y]\n",
    "            error = error * sample_weights \n",
    "        loss = -np.mean(error) \n",
    "        reg = self.lambda_reg * np.sum(self.W ** 2)\n",
    "        return loss + reg\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y, border=0.0001, class_weights=None):        \n",
    "        n_features = X.shape[1]\n",
    "        n_classes = len(np.unique(y))\n",
    "        print()\n",
    "        self.W = np.random.randn(n_features, n_classes) * 0.01\n",
    "        self.b = np.zeros(n_classes)        \n",
    "        self.class_weights = class_weights\n",
    "        y_onehot = np.eye(n_classes)[y]        \n",
    "        old_loss = float('inf')\n",
    "        for epoch in range(self.epochs):\n",
    "            probas = self.predict_probas(X)\n",
    "            loss = self._cross_entropy(y, y_onehot, probas)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print('Cross entropy:', loss)\n",
    "            if abs(old_loss - loss) < border:\n",
    "                break\n",
    "            old_loss = loss\n",
    "                  \n",
    "            error = y_onehot - probas\n",
    "            \n",
    "            if self.class_weights is not None:\n",
    "                sample_weights = np.array([self.class_weights[int(cl)] for cl in y])                \n",
    "                error = error * sample_weights\n",
    "            grad = -X.T.dot(error)\n",
    "            reg = self.lambda_reg * self.W \n",
    "            self.W -= self.alpha * (grad + reg)\n",
    "            self.b -= self.alpha * np.sum(error)\n",
    "        return self\n",
    "    def predict_probas(self, X):\n",
    "        z = X.dot(self.W) + self.b        \n",
    "        probas = self._softmax(z)\n",
    "        return probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_probas(X), axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14160059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross entropy: 0.36780571527398404\n",
      "Cross entropy: 0.1951824737473995\n",
      "Cross entropy: 0.25886903004785683\n",
      "Cross entropy: 0.3053251151988554\n",
      "Cross entropy: 0.3430909817905918\n",
      "Cross entropy: 0.37543110929851364\n",
      "Cross entropy: 0.40398227771431233\n",
      "Cross entropy: 0.429696187211784\n",
      "Cross entropy: 0.4531814801624235\n",
      "Cross entropy: 0.47485433423607565\n",
      "F1 of softmax: 0.9850746268656716\n",
      "Accuracy of softmax: 0.9850746268656716\n"
     ]
    }
   ],
   "source": [
    "softmax_classifier = LogisticRegression()\n",
    "softmax_classifier.fit(X_train, y_train)\n",
    "y_pred = softmax_classifier.predict(X_test)\n",
    "print('F1 of softmax:', f1_score(y_test, y_pred, average='micro'))\n",
    "print('Accuracy of softmax:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab20e8",
   "metadata": {},
   "source": [
    "So... the results of OvA and softmax are awesome and same.\n",
    "But I prefer the softmax because it produces a joint probability distribution that is guaranteed to sum to 1. This is crucial for tasks where calibrated probabilities are needed (e.g., risk assessment, predicting the probability of the next word in a language model).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54ca4afc",
      "metadata": {
        "id": "54ca4afc"
      },
      "source": [
        "## Implement word embeddings using CBOW from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8beccc0f",
      "metadata": {
        "id": "8beccc0f"
      },
      "source": [
        "### Import libs and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6bf33779",
      "metadata": {
        "id": "6bf33779"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6863ccf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6863ccf3",
        "outputId": "03843de9-b5aa-4615-93fa-2a70eadc1084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('imdb')\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "091e2900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "091e2900",
        "outputId": "4cace2b5-ef63-4014-e3a6-6930c72d892c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12500 12500]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "labels, counts = np.unique(train_data['label'], return_counts=True)\n",
        "print(counts)\n",
        "plt.hist(train_data['label'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### constants for cbow"
      ],
      "metadata": {
        "id": "KQNn4x8hkNkR"
      },
      "id": "KQNn4x8hkNkR"
    },
    {
      "cell_type": "code",
      "source": [
        "UNKNOWN_TOKEN = '<UNK>'\n",
        "PADDING_TOKEN = '<PAD>'\n",
        "BATCH_SIZE = 4096\n",
        "MIN_FREQUENCY = 5\n",
        "WINDOW_SIZE = 2"
      ],
      "metadata": {
        "id": "2UBnqS9pkRW2"
      },
      "id": "2UBnqS9pkRW2",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "960786ef",
      "metadata": {
        "id": "960786ef"
      },
      "source": [
        "#### tokenize text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "def tokenize(text):\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "words = tokenize(' '.join(train_data['text']))"
      ],
      "metadata": {
        "id": "B1jpCW4uhTYi"
      },
      "id": "B1jpCW4uhTYi",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "67d99cb2",
      "metadata": {
        "id": "67d99cb2"
      },
      "source": [
        "#### Create vocabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d3fea58a",
      "metadata": {
        "id": "d3fea58a"
      },
      "outputs": [],
      "source": [
        "def create_vocabs(words, min_frequency=5, pad_token='<PAD>', unk_token='<UNK>'):\n",
        "    words_counts = Counter(words)\n",
        "    word2idx = {unk_token: 0, pad_token: 1}\n",
        "    idx2word = {0: unk_token, 1: pad_token}\n",
        "    idx = 2\n",
        "    for word, count in words_counts.items():\n",
        "        if count> min_frequency:\n",
        "            word2idx[word] = idx\n",
        "            idx2word[idx] = word\n",
        "            idx += 1\n",
        "    return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = create_vocabs(words, min_frequency=MIN_FREQUENCY, pad_token=PADDING_TOKEN, unk_token=UNKNOWN_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68fb632",
      "metadata": {
        "id": "c68fb632"
      },
      "source": [
        "#### Create pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aac4ac50",
      "metadata": {
        "id": "aac4ac50"
      },
      "outputs": [],
      "source": [
        "def get_cbow_pairs(words, window_size=2):\n",
        "    pairs = []\n",
        "    n = len(words)\n",
        "    for i in range(n):\n",
        "        central_word = words[i] if words[i] in word2idx else UNKNOWN_TOKEN\n",
        "        if central_word not in word2idx:\n",
        "            central_word = UNKNOWN_TOKEN\n",
        "\n",
        "        # fill context\n",
        "        context = []\n",
        "        for j in range(i - window_size, i + window_size + 1):\n",
        "            context_word = PADDING_TOKEN\n",
        "            if j >= 0 and j < n and j != i:\n",
        "                context_word = words[j] if words[j] in word2idx else UNKNOWN_TOKEN\n",
        "            context.append(word2idx[context_word])\n",
        "\n",
        "        pairs.append((context, word2idx[central_word]))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "cbow_pairs = get_cbow_pairs(words, window_size=WINDOW_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83287afd",
      "metadata": {
        "id": "83287afd"
      },
      "source": [
        "#### Make dataset from pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fcccdd80",
      "metadata": {
        "id": "fcccdd80"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        super().__init__()\n",
        "        self.pairs = pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        context, central_word = self.pairs[idx]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(central_word, dtype=torch.long)\n",
        "\n",
        "cbow_dataset = CBOWDataset(cbow_pairs)\n",
        "train_cbow_dataloader = DataLoader(cbow_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "693ac946",
      "metadata": {
        "id": "693ac946"
      },
      "source": [
        "#### Implement CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "46ddcd5e",
      "metadata": {
        "id": "46ddcd5e"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, pad_idx=None):\n",
        "        super().__init__()\n",
        "        # self.context_embeddings = torch.randn(vocab_size, embedding_dim, requires_grad=True)\n",
        "        # self.central_embeddings = torch.randn(vocab_size, embedding_dim, requires_grad=True)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.central_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "    def forward(self, contexts_batch):\n",
        "          context_vectors = self.context_embeddings(contexts_batch).mean(dim=1)\n",
        "          logits = context_vectors @ self.central_embeddings.weight.T\n",
        "          return logits\n",
        "\n",
        "\n",
        "# working via collab\n",
        "gpu_t4 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "cbow = CBOW(len(word2idx), pad_idx=word2idx[PADDING_TOKEN]).to(gpu_t4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train CBOW"
      ],
      "metadata": {
        "id": "ok-DW98EcPEZ"
      },
      "id": "ok-DW98EcPEZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# for contexts_batch, central_words_batch in dataloader:\n",
        "#     contexts_batch = contexts_batch.to(gpu_t4)\n",
        "#     central_words_batch = central_words_batch.to(gpu_t4)\n",
        "#     cbow.forward(contexts_batch)\n",
        "#     break\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X,y = X.to(gpu_t4), y.to(gpu_t4)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 20 == 0:\n",
        "      loss, current = loss.item(), batch * BATCH_SIZE + len(X)\n",
        "      print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')"
      ],
      "metadata": {
        "id": "FtrOUaboYrD3"
      },
      "id": "FtrOUaboYrD3",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### make test data (make pipeline)"
      ],
      "metadata": {
        "id": "SOY3SC-KjK66"
      },
      "id": "SOY3SC-KjK66"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input_cbow(text):\n",
        "  words = tokenize(text)\n",
        "  pairs = get_cbow_pairs(words, window_size=WINDOW_SIZE)\n",
        "  dataset = CBOWDataset(pairs)\n",
        "  dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "iQaxsUKIjNQ6"
      },
      "id": "iQaxsUKIjNQ6",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cbow_dataloader = preprocess_input_cbow(' '.join(test_data['text']))\n",
        "cbow_loss_fn = nn.CrossEntropyLoss()\n",
        "cbow_optimizer = torch.optim.SGD(cbow.parameters(), lr=1e-2, momentum=0.8)\n",
        "for epoch in range(10):\n",
        "  print(f'\\n\\n\\n---------------------EPOCH {epoch}------------------\\n')\n",
        "  train_loop(train_cbow_dataloader, cbow, cbow_loss_fn, cbow_optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNTl-I1Zgoyu",
        "outputId": "0f3a0ca6-6620-4733-c8eb-19afb2e82187"
      },
      "id": "PNTl-I1Zgoyu",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 0------------------\n",
            "\n",
            "loss: 16.537220 [ 4096/6056873]\n",
            "loss: 16.489592 [86016/6056873]\n",
            "loss: 16.596195 [167936/6056873]\n",
            "loss: 16.539494 [249856/6056873]\n",
            "loss: 16.462257 [331776/6056873]\n",
            "loss: 16.484463 [413696/6056873]\n",
            "loss: 16.540308 [495616/6056873]\n",
            "loss: 16.420076 [577536/6056873]\n",
            "loss: 16.221577 [659456/6056873]\n",
            "loss: 16.518135 [741376/6056873]\n",
            "loss: 16.366102 [823296/6056873]\n",
            "loss: 16.362553 [905216/6056873]\n",
            "loss: 16.263205 [987136/6056873]\n",
            "loss: 16.329477 [1069056/6056873]\n",
            "loss: 16.314667 [1150976/6056873]\n",
            "loss: 16.298115 [1232896/6056873]\n",
            "loss: 16.354805 [1314816/6056873]\n",
            "loss: 16.273624 [1396736/6056873]\n",
            "loss: 16.415398 [1478656/6056873]\n",
            "loss: 16.274088 [1560576/6056873]\n",
            "loss: 16.188766 [1642496/6056873]\n",
            "loss: 16.172625 [1724416/6056873]\n",
            "loss: 16.212864 [1806336/6056873]\n",
            "loss: 16.302464 [1888256/6056873]\n",
            "loss: 16.316729 [1970176/6056873]\n",
            "loss: 16.168310 [2052096/6056873]\n",
            "loss: 16.183645 [2134016/6056873]\n",
            "loss: 16.166857 [2215936/6056873]\n",
            "loss: 16.093174 [2297856/6056873]\n",
            "loss: 16.167580 [2379776/6056873]\n",
            "loss: 16.166641 [2461696/6056873]\n",
            "loss: 16.066576 [2543616/6056873]\n",
            "loss: 16.140972 [2625536/6056873]\n",
            "loss: 16.063843 [2707456/6056873]\n",
            "loss: 16.031311 [2789376/6056873]\n",
            "loss: 16.121357 [2871296/6056873]\n",
            "loss: 16.107553 [2953216/6056873]\n",
            "loss: 16.039236 [3035136/6056873]\n",
            "loss: 16.101210 [3117056/6056873]\n",
            "loss: 16.079327 [3198976/6056873]\n",
            "loss: 16.065004 [3280896/6056873]\n",
            "loss: 15.899884 [3362816/6056873]\n",
            "loss: 15.916357 [3444736/6056873]\n",
            "loss: 15.914781 [3526656/6056873]\n",
            "loss: 15.918743 [3608576/6056873]\n",
            "loss: 15.999819 [3690496/6056873]\n",
            "loss: 15.867599 [3772416/6056873]\n",
            "loss: 16.016735 [3854336/6056873]\n",
            "loss: 15.970343 [3936256/6056873]\n",
            "loss: 15.837341 [4018176/6056873]\n",
            "loss: 15.912086 [4100096/6056873]\n",
            "loss: 15.946443 [4182016/6056873]\n",
            "loss: 15.859458 [4263936/6056873]\n",
            "loss: 15.844964 [4345856/6056873]\n",
            "loss: 15.695699 [4427776/6056873]\n",
            "loss: 15.877971 [4509696/6056873]\n",
            "loss: 15.776607 [4591616/6056873]\n",
            "loss: 15.818945 [4673536/6056873]\n",
            "loss: 15.795777 [4755456/6056873]\n",
            "loss: 15.800201 [4837376/6056873]\n",
            "loss: 15.836834 [4919296/6056873]\n",
            "loss: 15.685812 [5001216/6056873]\n",
            "loss: 15.759319 [5083136/6056873]\n",
            "loss: 15.697105 [5165056/6056873]\n",
            "loss: 15.727448 [5246976/6056873]\n",
            "loss: 15.771069 [5328896/6056873]\n",
            "loss: 15.713432 [5410816/6056873]\n",
            "loss: 15.742428 [5492736/6056873]\n",
            "loss: 15.591465 [5574656/6056873]\n",
            "loss: 15.670076 [5656576/6056873]\n",
            "loss: 15.825311 [5738496/6056873]\n",
            "loss: 15.844355 [5820416/6056873]\n",
            "loss: 15.557048 [5902336/6056873]\n",
            "loss: 15.582473 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 1------------------\n",
            "\n",
            "loss: 15.654051 [ 4096/6056873]\n",
            "loss: 15.486633 [86016/6056873]\n",
            "loss: 15.543207 [167936/6056873]\n",
            "loss: 15.592422 [249856/6056873]\n",
            "loss: 15.606411 [331776/6056873]\n",
            "loss: 15.568145 [413696/6056873]\n",
            "loss: 15.766006 [495616/6056873]\n",
            "loss: 15.602439 [577536/6056873]\n",
            "loss: 15.513406 [659456/6056873]\n",
            "loss: 15.497578 [741376/6056873]\n",
            "loss: 15.599049 [823296/6056873]\n",
            "loss: 15.568342 [905216/6056873]\n",
            "loss: 15.514434 [987136/6056873]\n",
            "loss: 15.443127 [1069056/6056873]\n",
            "loss: 15.529996 [1150976/6056873]\n",
            "loss: 15.506243 [1232896/6056873]\n",
            "loss: 15.481091 [1314816/6056873]\n",
            "loss: 15.390367 [1396736/6056873]\n",
            "loss: 15.461313 [1478656/6056873]\n",
            "loss: 15.499421 [1560576/6056873]\n",
            "loss: 15.364034 [1642496/6056873]\n",
            "loss: 15.307682 [1724416/6056873]\n",
            "loss: 15.433139 [1806336/6056873]\n",
            "loss: 15.381422 [1888256/6056873]\n",
            "loss: 15.399067 [1970176/6056873]\n",
            "loss: 15.355975 [2052096/6056873]\n",
            "loss: 15.367188 [2134016/6056873]\n",
            "loss: 15.290615 [2215936/6056873]\n",
            "loss: 15.413497 [2297856/6056873]\n",
            "loss: 15.378613 [2379776/6056873]\n",
            "loss: 15.268888 [2461696/6056873]\n",
            "loss: 15.383243 [2543616/6056873]\n",
            "loss: 15.303471 [2625536/6056873]\n",
            "loss: 15.359483 [2707456/6056873]\n",
            "loss: 15.341906 [2789376/6056873]\n",
            "loss: 15.367416 [2871296/6056873]\n",
            "loss: 15.266754 [2953216/6056873]\n",
            "loss: 15.230961 [3035136/6056873]\n",
            "loss: 15.344625 [3117056/6056873]\n",
            "loss: 15.204640 [3198976/6056873]\n",
            "loss: 15.231308 [3280896/6056873]\n",
            "loss: 15.275505 [3362816/6056873]\n",
            "loss: 15.213702 [3444736/6056873]\n",
            "loss: 15.373596 [3526656/6056873]\n",
            "loss: 15.258951 [3608576/6056873]\n",
            "loss: 15.314267 [3690496/6056873]\n",
            "loss: 15.234771 [3772416/6056873]\n",
            "loss: 15.152893 [3854336/6056873]\n",
            "loss: 15.198644 [3936256/6056873]\n",
            "loss: 15.176397 [4018176/6056873]\n",
            "loss: 15.209542 [4100096/6056873]\n",
            "loss: 15.227131 [4182016/6056873]\n",
            "loss: 15.159960 [4263936/6056873]\n",
            "loss: 15.078212 [4345856/6056873]\n",
            "loss: 15.090962 [4427776/6056873]\n",
            "loss: 15.016075 [4509696/6056873]\n",
            "loss: 15.104292 [4591616/6056873]\n",
            "loss: 15.124395 [4673536/6056873]\n",
            "loss: 15.258415 [4755456/6056873]\n",
            "loss: 15.125821 [4837376/6056873]\n",
            "loss: 15.153907 [4919296/6056873]\n",
            "loss: 15.131386 [5001216/6056873]\n",
            "loss: 15.056968 [5083136/6056873]\n",
            "loss: 15.130681 [5165056/6056873]\n",
            "loss: 14.969098 [5246976/6056873]\n",
            "loss: 15.111932 [5328896/6056873]\n",
            "loss: 15.132067 [5410816/6056873]\n",
            "loss: 15.100227 [5492736/6056873]\n",
            "loss: 15.050465 [5574656/6056873]\n",
            "loss: 15.038387 [5656576/6056873]\n",
            "loss: 14.939836 [5738496/6056873]\n",
            "loss: 14.962406 [5820416/6056873]\n",
            "loss: 15.167396 [5902336/6056873]\n",
            "loss: 14.883237 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 2------------------\n",
            "\n",
            "loss: 14.942000 [ 4096/6056873]\n",
            "loss: 15.062535 [86016/6056873]\n",
            "loss: 14.958864 [167936/6056873]\n",
            "loss: 14.848562 [249856/6056873]\n",
            "loss: 14.857779 [331776/6056873]\n",
            "loss: 14.946580 [413696/6056873]\n",
            "loss: 14.990937 [495616/6056873]\n",
            "loss: 14.902374 [577536/6056873]\n",
            "loss: 14.914011 [659456/6056873]\n",
            "loss: 14.761284 [741376/6056873]\n",
            "loss: 14.957860 [823296/6056873]\n",
            "loss: 14.925603 [905216/6056873]\n",
            "loss: 14.926343 [987136/6056873]\n",
            "loss: 14.741949 [1069056/6056873]\n",
            "loss: 14.955159 [1150976/6056873]\n",
            "loss: 14.784217 [1232896/6056873]\n",
            "loss: 14.864755 [1314816/6056873]\n",
            "loss: 14.770187 [1396736/6056873]\n",
            "loss: 14.779275 [1478656/6056873]\n",
            "loss: 14.838684 [1560576/6056873]\n",
            "loss: 14.925050 [1642496/6056873]\n",
            "loss: 14.970270 [1724416/6056873]\n",
            "loss: 14.808378 [1806336/6056873]\n",
            "loss: 14.972737 [1888256/6056873]\n",
            "loss: 14.857841 [1970176/6056873]\n",
            "loss: 14.777708 [2052096/6056873]\n",
            "loss: 14.822407 [2134016/6056873]\n",
            "loss: 14.730369 [2215936/6056873]\n",
            "loss: 14.904949 [2297856/6056873]\n",
            "loss: 14.797479 [2379776/6056873]\n",
            "loss: 14.829806 [2461696/6056873]\n",
            "loss: 14.770035 [2543616/6056873]\n",
            "loss: 14.809649 [2625536/6056873]\n",
            "loss: 14.698065 [2707456/6056873]\n",
            "loss: 14.672700 [2789376/6056873]\n",
            "loss: 14.729627 [2871296/6056873]\n",
            "loss: 14.675123 [2953216/6056873]\n",
            "loss: 14.638223 [3035136/6056873]\n",
            "loss: 14.725083 [3117056/6056873]\n",
            "loss: 14.574401 [3198976/6056873]\n",
            "loss: 14.529737 [3280896/6056873]\n",
            "loss: 14.758471 [3362816/6056873]\n",
            "loss: 14.780177 [3444736/6056873]\n",
            "loss: 14.666671 [3526656/6056873]\n",
            "loss: 14.639842 [3608576/6056873]\n",
            "loss: 14.614828 [3690496/6056873]\n",
            "loss: 14.711362 [3772416/6056873]\n",
            "loss: 14.760778 [3854336/6056873]\n",
            "loss: 14.717844 [3936256/6056873]\n",
            "loss: 14.677800 [4018176/6056873]\n",
            "loss: 14.608588 [4100096/6056873]\n",
            "loss: 14.559886 [4182016/6056873]\n",
            "loss: 14.637124 [4263936/6056873]\n",
            "loss: 14.542608 [4345856/6056873]\n",
            "loss: 14.523362 [4427776/6056873]\n",
            "loss: 14.598742 [4509696/6056873]\n",
            "loss: 14.728809 [4591616/6056873]\n",
            "loss: 14.700295 [4673536/6056873]\n",
            "loss: 14.653128 [4755456/6056873]\n",
            "loss: 14.600026 [4837376/6056873]\n",
            "loss: 14.526338 [4919296/6056873]\n",
            "loss: 14.501127 [5001216/6056873]\n",
            "loss: 14.702318 [5083136/6056873]\n",
            "loss: 14.569134 [5165056/6056873]\n",
            "loss: 14.567941 [5246976/6056873]\n",
            "loss: 14.562201 [5328896/6056873]\n",
            "loss: 14.519479 [5410816/6056873]\n",
            "loss: 14.552322 [5492736/6056873]\n",
            "loss: 14.525809 [5574656/6056873]\n",
            "loss: 14.522985 [5656576/6056873]\n",
            "loss: 14.381203 [5738496/6056873]\n",
            "loss: 14.561810 [5820416/6056873]\n",
            "loss: 14.549991 [5902336/6056873]\n",
            "loss: 14.483017 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 3------------------\n",
            "\n",
            "loss: 14.604260 [ 4096/6056873]\n",
            "loss: 14.683743 [86016/6056873]\n",
            "loss: 14.366620 [167936/6056873]\n",
            "loss: 14.428663 [249856/6056873]\n",
            "loss: 14.453932 [331776/6056873]\n",
            "loss: 14.452751 [413696/6056873]\n",
            "loss: 14.356967 [495616/6056873]\n",
            "loss: 14.560478 [577536/6056873]\n",
            "loss: 14.486537 [659456/6056873]\n",
            "loss: 14.398125 [741376/6056873]\n",
            "loss: 14.371413 [823296/6056873]\n",
            "loss: 14.506193 [905216/6056873]\n",
            "loss: 14.368383 [987136/6056873]\n",
            "loss: 14.310611 [1069056/6056873]\n",
            "loss: 14.233560 [1150976/6056873]\n",
            "loss: 14.373462 [1232896/6056873]\n",
            "loss: 14.311661 [1314816/6056873]\n",
            "loss: 14.339140 [1396736/6056873]\n",
            "loss: 14.223835 [1478656/6056873]\n",
            "loss: 14.198580 [1560576/6056873]\n",
            "loss: 14.273388 [1642496/6056873]\n",
            "loss: 14.357677 [1724416/6056873]\n",
            "loss: 14.533322 [1806336/6056873]\n",
            "loss: 14.440523 [1888256/6056873]\n",
            "loss: 14.320267 [1970176/6056873]\n",
            "loss: 14.261200 [2052096/6056873]\n",
            "loss: 14.437288 [2134016/6056873]\n",
            "loss: 14.331551 [2215936/6056873]\n",
            "loss: 14.320312 [2297856/6056873]\n",
            "loss: 14.272925 [2379776/6056873]\n",
            "loss: 14.378324 [2461696/6056873]\n",
            "loss: 14.248302 [2543616/6056873]\n",
            "loss: 14.187126 [2625536/6056873]\n",
            "loss: 14.210787 [2707456/6056873]\n",
            "loss: 14.415990 [2789376/6056873]\n",
            "loss: 14.199350 [2871296/6056873]\n",
            "loss: 14.249393 [2953216/6056873]\n",
            "loss: 14.390061 [3035136/6056873]\n",
            "loss: 14.256006 [3117056/6056873]\n",
            "loss: 14.265070 [3198976/6056873]\n",
            "loss: 14.279343 [3280896/6056873]\n",
            "loss: 14.192052 [3362816/6056873]\n",
            "loss: 14.321265 [3444736/6056873]\n",
            "loss: 14.289285 [3526656/6056873]\n",
            "loss: 14.144729 [3608576/6056873]\n",
            "loss: 14.297973 [3690496/6056873]\n",
            "loss: 14.194006 [3772416/6056873]\n",
            "loss: 14.210553 [3854336/6056873]\n",
            "loss: 14.180762 [3936256/6056873]\n",
            "loss: 14.280068 [4018176/6056873]\n",
            "loss: 14.212828 [4100096/6056873]\n",
            "loss: 14.138203 [4182016/6056873]\n",
            "loss: 14.304515 [4263936/6056873]\n",
            "loss: 14.138821 [4345856/6056873]\n",
            "loss: 14.195770 [4427776/6056873]\n",
            "loss: 14.091104 [4509696/6056873]\n",
            "loss: 14.159136 [4591616/6056873]\n",
            "loss: 14.136957 [4673536/6056873]\n",
            "loss: 14.121834 [4755456/6056873]\n",
            "loss: 14.199559 [4837376/6056873]\n",
            "loss: 14.196029 [4919296/6056873]\n",
            "loss: 14.155910 [5001216/6056873]\n",
            "loss: 14.260097 [5083136/6056873]\n",
            "loss: 13.965075 [5165056/6056873]\n",
            "loss: 14.153752 [5246976/6056873]\n",
            "loss: 14.038812 [5328896/6056873]\n",
            "loss: 14.043619 [5410816/6056873]\n",
            "loss: 14.053257 [5492736/6056873]\n",
            "loss: 14.097850 [5574656/6056873]\n",
            "loss: 14.123811 [5656576/6056873]\n",
            "loss: 14.052006 [5738496/6056873]\n",
            "loss: 14.150208 [5820416/6056873]\n",
            "loss: 14.116109 [5902336/6056873]\n",
            "loss: 13.985512 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 4------------------\n",
            "\n",
            "loss: 14.042581 [ 4096/6056873]\n",
            "loss: 14.074475 [86016/6056873]\n",
            "loss: 14.159162 [167936/6056873]\n",
            "loss: 14.091347 [249856/6056873]\n",
            "loss: 14.020663 [331776/6056873]\n",
            "loss: 14.032067 [413696/6056873]\n",
            "loss: 14.118382 [495616/6056873]\n",
            "loss: 14.141011 [577536/6056873]\n",
            "loss: 13.973704 [659456/6056873]\n",
            "loss: 14.014092 [741376/6056873]\n",
            "loss: 13.930453 [823296/6056873]\n",
            "loss: 14.006682 [905216/6056873]\n",
            "loss: 14.009324 [987136/6056873]\n",
            "loss: 14.016356 [1069056/6056873]\n",
            "loss: 13.996170 [1150976/6056873]\n",
            "loss: 14.027351 [1232896/6056873]\n",
            "loss: 13.873470 [1314816/6056873]\n",
            "loss: 13.896272 [1396736/6056873]\n",
            "loss: 13.911023 [1478656/6056873]\n",
            "loss: 13.948977 [1560576/6056873]\n",
            "loss: 13.922318 [1642496/6056873]\n",
            "loss: 13.828702 [1724416/6056873]\n",
            "loss: 14.023260 [1806336/6056873]\n",
            "loss: 13.898571 [1888256/6056873]\n",
            "loss: 13.943387 [1970176/6056873]\n",
            "loss: 13.802504 [2052096/6056873]\n",
            "loss: 14.006847 [2134016/6056873]\n",
            "loss: 13.753802 [2215936/6056873]\n",
            "loss: 13.912487 [2297856/6056873]\n",
            "loss: 13.996511 [2379776/6056873]\n",
            "loss: 13.912468 [2461696/6056873]\n",
            "loss: 13.850944 [2543616/6056873]\n",
            "loss: 13.877752 [2625536/6056873]\n",
            "loss: 13.822936 [2707456/6056873]\n",
            "loss: 13.839901 [2789376/6056873]\n",
            "loss: 13.857146 [2871296/6056873]\n",
            "loss: 13.916507 [2953216/6056873]\n",
            "loss: 13.910665 [3035136/6056873]\n",
            "loss: 13.780310 [3117056/6056873]\n",
            "loss: 13.877785 [3198976/6056873]\n",
            "loss: 13.871716 [3280896/6056873]\n",
            "loss: 13.832332 [3362816/6056873]\n",
            "loss: 13.829612 [3444736/6056873]\n",
            "loss: 13.716616 [3526656/6056873]\n",
            "loss: 13.888808 [3608576/6056873]\n",
            "loss: 13.866611 [3690496/6056873]\n",
            "loss: 13.801170 [3772416/6056873]\n",
            "loss: 13.865368 [3854336/6056873]\n",
            "loss: 13.860561 [3936256/6056873]\n",
            "loss: 13.810528 [4018176/6056873]\n",
            "loss: 13.737711 [4100096/6056873]\n",
            "loss: 13.775403 [4182016/6056873]\n",
            "loss: 13.854066 [4263936/6056873]\n",
            "loss: 13.699157 [4345856/6056873]\n",
            "loss: 13.942290 [4427776/6056873]\n",
            "loss: 13.756324 [4509696/6056873]\n",
            "loss: 13.776112 [4591616/6056873]\n",
            "loss: 13.720969 [4673536/6056873]\n",
            "loss: 13.714720 [4755456/6056873]\n",
            "loss: 13.689007 [4837376/6056873]\n",
            "loss: 13.858577 [4919296/6056873]\n",
            "loss: 13.835163 [5001216/6056873]\n",
            "loss: 13.698505 [5083136/6056873]\n",
            "loss: 13.781894 [5165056/6056873]\n",
            "loss: 13.865993 [5246976/6056873]\n",
            "loss: 13.747438 [5328896/6056873]\n",
            "loss: 13.860139 [5410816/6056873]\n",
            "loss: 13.798154 [5492736/6056873]\n",
            "loss: 13.649094 [5574656/6056873]\n",
            "loss: 13.639260 [5656576/6056873]\n",
            "loss: 13.766455 [5738496/6056873]\n",
            "loss: 13.660245 [5820416/6056873]\n",
            "loss: 13.730571 [5902336/6056873]\n",
            "loss: 13.630691 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 5------------------\n",
            "\n",
            "loss: 13.671491 [ 4096/6056873]\n",
            "loss: 13.674573 [86016/6056873]\n",
            "loss: 13.746514 [167936/6056873]\n",
            "loss: 13.713296 [249856/6056873]\n",
            "loss: 13.709134 [331776/6056873]\n",
            "loss: 13.624794 [413696/6056873]\n",
            "loss: 13.745654 [495616/6056873]\n",
            "loss: 13.536928 [577536/6056873]\n",
            "loss: 13.648960 [659456/6056873]\n",
            "loss: 13.578154 [741376/6056873]\n",
            "loss: 13.586955 [823296/6056873]\n",
            "loss: 13.490826 [905216/6056873]\n",
            "loss: 13.680800 [987136/6056873]\n",
            "loss: 13.679475 [1069056/6056873]\n",
            "loss: 13.670118 [1150976/6056873]\n",
            "loss: 13.686991 [1232896/6056873]\n",
            "loss: 13.676863 [1314816/6056873]\n",
            "loss: 13.618017 [1396736/6056873]\n",
            "loss: 13.601063 [1478656/6056873]\n",
            "loss: 13.556540 [1560576/6056873]\n",
            "loss: 13.510918 [1642496/6056873]\n",
            "loss: 13.844034 [1724416/6056873]\n",
            "loss: 13.572065 [1806336/6056873]\n",
            "loss: 13.653965 [1888256/6056873]\n",
            "loss: 13.664032 [1970176/6056873]\n",
            "loss: 13.695326 [2052096/6056873]\n",
            "loss: 13.630451 [2134016/6056873]\n",
            "loss: 13.702070 [2215936/6056873]\n",
            "loss: 13.500700 [2297856/6056873]\n",
            "loss: 13.608250 [2379776/6056873]\n",
            "loss: 13.553515 [2461696/6056873]\n",
            "loss: 13.509007 [2543616/6056873]\n",
            "loss: 13.618896 [2625536/6056873]\n",
            "loss: 13.667341 [2707456/6056873]\n",
            "loss: 13.566696 [2789376/6056873]\n",
            "loss: 13.518562 [2871296/6056873]\n",
            "loss: 13.525767 [2953216/6056873]\n",
            "loss: 13.609666 [3035136/6056873]\n",
            "loss: 13.328477 [3117056/6056873]\n",
            "loss: 13.530568 [3198976/6056873]\n",
            "loss: 13.467291 [3280896/6056873]\n",
            "loss: 13.588605 [3362816/6056873]\n",
            "loss: 13.507098 [3444736/6056873]\n",
            "loss: 13.360187 [3526656/6056873]\n",
            "loss: 13.474939 [3608576/6056873]\n",
            "loss: 13.399048 [3690496/6056873]\n",
            "loss: 13.426841 [3772416/6056873]\n",
            "loss: 13.527103 [3854336/6056873]\n",
            "loss: 13.516881 [3936256/6056873]\n",
            "loss: 13.314894 [4018176/6056873]\n",
            "loss: 13.417444 [4100096/6056873]\n",
            "loss: 13.516836 [4182016/6056873]\n",
            "loss: 13.447215 [4263936/6056873]\n",
            "loss: 13.449598 [4345856/6056873]\n",
            "loss: 13.359102 [4427776/6056873]\n",
            "loss: 13.507545 [4509696/6056873]\n",
            "loss: 13.434428 [4591616/6056873]\n",
            "loss: 13.409531 [4673536/6056873]\n",
            "loss: 13.657599 [4755456/6056873]\n",
            "loss: 13.386155 [4837376/6056873]\n",
            "loss: 13.360929 [4919296/6056873]\n",
            "loss: 13.458378 [5001216/6056873]\n",
            "loss: 13.309924 [5083136/6056873]\n",
            "loss: 13.440298 [5165056/6056873]\n",
            "loss: 13.391099 [5246976/6056873]\n",
            "loss: 13.441641 [5328896/6056873]\n",
            "loss: 13.294968 [5410816/6056873]\n",
            "loss: 13.514928 [5492736/6056873]\n",
            "loss: 13.508387 [5574656/6056873]\n",
            "loss: 13.386269 [5656576/6056873]\n",
            "loss: 13.536314 [5738496/6056873]\n",
            "loss: 13.434424 [5820416/6056873]\n",
            "loss: 13.407666 [5902336/6056873]\n",
            "loss: 13.386744 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 6------------------\n",
            "\n",
            "loss: 13.349460 [ 4096/6056873]\n",
            "loss: 13.439400 [86016/6056873]\n",
            "loss: 13.485450 [167936/6056873]\n",
            "loss: 13.364967 [249856/6056873]\n",
            "loss: 13.352012 [331776/6056873]\n",
            "loss: 13.236829 [413696/6056873]\n",
            "loss: 13.379715 [495616/6056873]\n",
            "loss: 13.355470 [577536/6056873]\n",
            "loss: 13.513926 [659456/6056873]\n",
            "loss: 13.321481 [741376/6056873]\n",
            "loss: 13.299707 [823296/6056873]\n",
            "loss: 13.388372 [905216/6056873]\n",
            "loss: 13.447956 [987136/6056873]\n",
            "loss: 13.489122 [1069056/6056873]\n",
            "loss: 13.420523 [1150976/6056873]\n",
            "loss: 13.379072 [1232896/6056873]\n",
            "loss: 13.329052 [1314816/6056873]\n",
            "loss: 13.141883 [1396736/6056873]\n",
            "loss: 13.443885 [1478656/6056873]\n",
            "loss: 13.392118 [1560576/6056873]\n",
            "loss: 13.242897 [1642496/6056873]\n",
            "loss: 13.292700 [1724416/6056873]\n",
            "loss: 13.376572 [1806336/6056873]\n",
            "loss: 13.293168 [1888256/6056873]\n",
            "loss: 13.426227 [1970176/6056873]\n",
            "loss: 13.360398 [2052096/6056873]\n",
            "loss: 13.350647 [2134016/6056873]\n",
            "loss: 13.338421 [2215936/6056873]\n",
            "loss: 13.265022 [2297856/6056873]\n",
            "loss: 13.275606 [2379776/6056873]\n",
            "loss: 13.190512 [2461696/6056873]\n",
            "loss: 13.236691 [2543616/6056873]\n",
            "loss: 13.098026 [2625536/6056873]\n",
            "loss: 13.222232 [2707456/6056873]\n",
            "loss: 13.231776 [2789376/6056873]\n",
            "loss: 13.320327 [2871296/6056873]\n",
            "loss: 13.159861 [2953216/6056873]\n",
            "loss: 13.237494 [3035136/6056873]\n",
            "loss: 13.295331 [3117056/6056873]\n",
            "loss: 13.152597 [3198976/6056873]\n",
            "loss: 13.256428 [3280896/6056873]\n",
            "loss: 13.130131 [3362816/6056873]\n",
            "loss: 13.208640 [3444736/6056873]\n",
            "loss: 13.056614 [3526656/6056873]\n",
            "loss: 13.249794 [3608576/6056873]\n",
            "loss: 13.269108 [3690496/6056873]\n",
            "loss: 13.196927 [3772416/6056873]\n",
            "loss: 13.215566 [3854336/6056873]\n",
            "loss: 13.174671 [3936256/6056873]\n",
            "loss: 13.172443 [4018176/6056873]\n",
            "loss: 13.191132 [4100096/6056873]\n",
            "loss: 13.156903 [4182016/6056873]\n",
            "loss: 13.222680 [4263936/6056873]\n",
            "loss: 13.206726 [4345856/6056873]\n",
            "loss: 13.181043 [4427776/6056873]\n",
            "loss: 13.288329 [4509696/6056873]\n",
            "loss: 13.179728 [4591616/6056873]\n",
            "loss: 13.239532 [4673536/6056873]\n",
            "loss: 13.163161 [4755456/6056873]\n",
            "loss: 13.221619 [4837376/6056873]\n",
            "loss: 13.152834 [4919296/6056873]\n",
            "loss: 13.107633 [5001216/6056873]\n",
            "loss: 13.083889 [5083136/6056873]\n",
            "loss: 13.115402 [5165056/6056873]\n",
            "loss: 13.136783 [5246976/6056873]\n",
            "loss: 13.149085 [5328896/6056873]\n",
            "loss: 13.166576 [5410816/6056873]\n",
            "loss: 13.170014 [5492736/6056873]\n",
            "loss: 13.149329 [5574656/6056873]\n",
            "loss: 13.053267 [5656576/6056873]\n",
            "loss: 13.211708 [5738496/6056873]\n",
            "loss: 13.082546 [5820416/6056873]\n",
            "loss: 13.284062 [5902336/6056873]\n",
            "loss: 13.086430 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 7------------------\n",
            "\n",
            "loss: 13.108566 [ 4096/6056873]\n",
            "loss: 13.188669 [86016/6056873]\n",
            "loss: 13.084638 [167936/6056873]\n",
            "loss: 13.185454 [249856/6056873]\n",
            "loss: 13.033106 [331776/6056873]\n",
            "loss: 13.102021 [413696/6056873]\n",
            "loss: 13.164393 [495616/6056873]\n",
            "loss: 13.049205 [577536/6056873]\n",
            "loss: 13.119090 [659456/6056873]\n",
            "loss: 13.079273 [741376/6056873]\n",
            "loss: 13.064026 [823296/6056873]\n",
            "loss: 13.111758 [905216/6056873]\n",
            "loss: 13.100169 [987136/6056873]\n",
            "loss: 13.104492 [1069056/6056873]\n",
            "loss: 13.028934 [1150976/6056873]\n",
            "loss: 12.967150 [1232896/6056873]\n",
            "loss: 13.086440 [1314816/6056873]\n",
            "loss: 13.059844 [1396736/6056873]\n",
            "loss: 13.134397 [1478656/6056873]\n",
            "loss: 13.079344 [1560576/6056873]\n",
            "loss: 13.239382 [1642496/6056873]\n",
            "loss: 13.040564 [1724416/6056873]\n",
            "loss: 13.115477 [1806336/6056873]\n",
            "loss: 13.126640 [1888256/6056873]\n",
            "loss: 12.964687 [1970176/6056873]\n",
            "loss: 12.945484 [2052096/6056873]\n",
            "loss: 12.918408 [2134016/6056873]\n",
            "loss: 12.991489 [2215936/6056873]\n",
            "loss: 13.080818 [2297856/6056873]\n",
            "loss: 13.030244 [2379776/6056873]\n",
            "loss: 13.033311 [2461696/6056873]\n",
            "loss: 12.992146 [2543616/6056873]\n",
            "loss: 12.933958 [2625536/6056873]\n",
            "loss: 13.041265 [2707456/6056873]\n",
            "loss: 12.933724 [2789376/6056873]\n",
            "loss: 12.937366 [2871296/6056873]\n",
            "loss: 12.963396 [2953216/6056873]\n",
            "loss: 12.976509 [3035136/6056873]\n",
            "loss: 13.076298 [3117056/6056873]\n",
            "loss: 12.928798 [3198976/6056873]\n",
            "loss: 12.913274 [3280896/6056873]\n",
            "loss: 12.899582 [3362816/6056873]\n",
            "loss: 13.125669 [3444736/6056873]\n",
            "loss: 13.001695 [3526656/6056873]\n",
            "loss: 13.074284 [3608576/6056873]\n",
            "loss: 12.988792 [3690496/6056873]\n",
            "loss: 13.036281 [3772416/6056873]\n",
            "loss: 13.055893 [3854336/6056873]\n",
            "loss: 12.991745 [3936256/6056873]\n",
            "loss: 13.067258 [4018176/6056873]\n",
            "loss: 13.004527 [4100096/6056873]\n",
            "loss: 12.935715 [4182016/6056873]\n",
            "loss: 12.981693 [4263936/6056873]\n",
            "loss: 12.936222 [4345856/6056873]\n",
            "loss: 13.026525 [4427776/6056873]\n",
            "loss: 12.929068 [4509696/6056873]\n",
            "loss: 12.981224 [4591616/6056873]\n",
            "loss: 12.887828 [4673536/6056873]\n",
            "loss: 12.815729 [4755456/6056873]\n",
            "loss: 13.017562 [4837376/6056873]\n",
            "loss: 12.871681 [4919296/6056873]\n",
            "loss: 13.010659 [5001216/6056873]\n",
            "loss: 13.019628 [5083136/6056873]\n",
            "loss: 12.884729 [5165056/6056873]\n",
            "loss: 12.892649 [5246976/6056873]\n",
            "loss: 12.928692 [5328896/6056873]\n",
            "loss: 12.910693 [5410816/6056873]\n",
            "loss: 12.855678 [5492736/6056873]\n",
            "loss: 12.907650 [5574656/6056873]\n",
            "loss: 12.855914 [5656576/6056873]\n",
            "loss: 12.844129 [5738496/6056873]\n",
            "loss: 12.916479 [5820416/6056873]\n",
            "loss: 12.874975 [5902336/6056873]\n",
            "loss: 12.959387 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 8------------------\n",
            "\n",
            "loss: 12.861388 [ 4096/6056873]\n",
            "loss: 12.846183 [86016/6056873]\n",
            "loss: 12.803423 [167936/6056873]\n",
            "loss: 12.895782 [249856/6056873]\n",
            "loss: 12.850810 [331776/6056873]\n",
            "loss: 12.965647 [413696/6056873]\n",
            "loss: 12.866520 [495616/6056873]\n",
            "loss: 12.933291 [577536/6056873]\n",
            "loss: 12.846128 [659456/6056873]\n",
            "loss: 12.847039 [741376/6056873]\n",
            "loss: 12.898656 [823296/6056873]\n",
            "loss: 12.913797 [905216/6056873]\n",
            "loss: 12.793610 [987136/6056873]\n",
            "loss: 12.993483 [1069056/6056873]\n",
            "loss: 12.922394 [1150976/6056873]\n",
            "loss: 12.852620 [1232896/6056873]\n",
            "loss: 12.736658 [1314816/6056873]\n",
            "loss: 12.952018 [1396736/6056873]\n",
            "loss: 12.844617 [1478656/6056873]\n",
            "loss: 12.781969 [1560576/6056873]\n",
            "loss: 12.871084 [1642496/6056873]\n",
            "loss: 12.916005 [1724416/6056873]\n",
            "loss: 12.856014 [1806336/6056873]\n",
            "loss: 12.765954 [1888256/6056873]\n",
            "loss: 12.863242 [1970176/6056873]\n",
            "loss: 12.879707 [2052096/6056873]\n",
            "loss: 12.786480 [2134016/6056873]\n",
            "loss: 12.842618 [2215936/6056873]\n",
            "loss: 12.795717 [2297856/6056873]\n",
            "loss: 12.888465 [2379776/6056873]\n",
            "loss: 12.833324 [2461696/6056873]\n",
            "loss: 12.737794 [2543616/6056873]\n",
            "loss: 12.754303 [2625536/6056873]\n",
            "loss: 12.725244 [2707456/6056873]\n",
            "loss: 12.848593 [2789376/6056873]\n",
            "loss: 12.740364 [2871296/6056873]\n",
            "loss: 12.894024 [2953216/6056873]\n",
            "loss: 12.813568 [3035136/6056873]\n",
            "loss: 12.822821 [3117056/6056873]\n",
            "loss: 12.858657 [3198976/6056873]\n",
            "loss: 12.936323 [3280896/6056873]\n",
            "loss: 12.859248 [3362816/6056873]\n",
            "loss: 12.647831 [3444736/6056873]\n",
            "loss: 12.748835 [3526656/6056873]\n",
            "loss: 12.688975 [3608576/6056873]\n",
            "loss: 12.837390 [3690496/6056873]\n",
            "loss: 12.744460 [3772416/6056873]\n",
            "loss: 12.695451 [3854336/6056873]\n",
            "loss: 12.596845 [3936256/6056873]\n",
            "loss: 12.810515 [4018176/6056873]\n",
            "loss: 12.901485 [4100096/6056873]\n",
            "loss: 12.738160 [4182016/6056873]\n",
            "loss: 12.644014 [4263936/6056873]\n",
            "loss: 12.662070 [4345856/6056873]\n",
            "loss: 12.666624 [4427776/6056873]\n",
            "loss: 12.660167 [4509696/6056873]\n",
            "loss: 12.800124 [4591616/6056873]\n",
            "loss: 12.758217 [4673536/6056873]\n",
            "loss: 12.660257 [4755456/6056873]\n",
            "loss: 12.654198 [4837376/6056873]\n",
            "loss: 12.752872 [4919296/6056873]\n",
            "loss: 12.699561 [5001216/6056873]\n",
            "loss: 12.711675 [5083136/6056873]\n",
            "loss: 12.698816 [5165056/6056873]\n",
            "loss: 12.717969 [5246976/6056873]\n",
            "loss: 12.649368 [5328896/6056873]\n",
            "loss: 12.692464 [5410816/6056873]\n",
            "loss: 12.544455 [5492736/6056873]\n",
            "loss: 12.778087 [5574656/6056873]\n",
            "loss: 12.814822 [5656576/6056873]\n",
            "loss: 12.749167 [5738496/6056873]\n",
            "loss: 12.652051 [5820416/6056873]\n",
            "loss: 12.703869 [5902336/6056873]\n",
            "loss: 12.701353 [5984256/6056873]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 9------------------\n",
            "\n",
            "loss: 12.525770 [ 4096/6056873]\n",
            "loss: 12.647496 [86016/6056873]\n",
            "loss: 12.616681 [167936/6056873]\n",
            "loss: 12.827238 [249856/6056873]\n",
            "loss: 12.679021 [331776/6056873]\n",
            "loss: 12.666780 [413696/6056873]\n",
            "loss: 12.658377 [495616/6056873]\n",
            "loss: 12.677960 [577536/6056873]\n",
            "loss: 12.595762 [659456/6056873]\n",
            "loss: 12.742136 [741376/6056873]\n",
            "loss: 12.661791 [823296/6056873]\n",
            "loss: 12.617485 [905216/6056873]\n",
            "loss: 12.805889 [987136/6056873]\n",
            "loss: 12.642723 [1069056/6056873]\n",
            "loss: 12.598189 [1150976/6056873]\n",
            "loss: 12.479684 [1232896/6056873]\n",
            "loss: 12.496914 [1314816/6056873]\n",
            "loss: 12.856298 [1396736/6056873]\n",
            "loss: 12.600996 [1478656/6056873]\n",
            "loss: 12.672664 [1560576/6056873]\n",
            "loss: 12.768568 [1642496/6056873]\n",
            "loss: 12.624401 [1724416/6056873]\n",
            "loss: 12.624357 [1806336/6056873]\n",
            "loss: 12.542128 [1888256/6056873]\n",
            "loss: 12.690664 [1970176/6056873]\n",
            "loss: 12.648379 [2052096/6056873]\n",
            "loss: 12.633894 [2134016/6056873]\n",
            "loss: 12.591808 [2215936/6056873]\n",
            "loss: 12.503494 [2297856/6056873]\n",
            "loss: 12.688596 [2379776/6056873]\n",
            "loss: 12.669687 [2461696/6056873]\n",
            "loss: 12.543709 [2543616/6056873]\n",
            "loss: 12.728382 [2625536/6056873]\n",
            "loss: 12.595295 [2707456/6056873]\n",
            "loss: 12.730357 [2789376/6056873]\n",
            "loss: 12.495451 [2871296/6056873]\n",
            "loss: 12.520088 [2953216/6056873]\n",
            "loss: 12.710743 [3035136/6056873]\n",
            "loss: 12.596573 [3117056/6056873]\n",
            "loss: 12.781328 [3198976/6056873]\n",
            "loss: 12.615764 [3280896/6056873]\n",
            "loss: 12.532863 [3362816/6056873]\n",
            "loss: 12.693975 [3444736/6056873]\n",
            "loss: 12.569864 [3526656/6056873]\n",
            "loss: 12.498727 [3608576/6056873]\n",
            "loss: 12.552006 [3690496/6056873]\n",
            "loss: 12.597239 [3772416/6056873]\n",
            "loss: 12.628613 [3854336/6056873]\n",
            "loss: 12.611382 [3936256/6056873]\n",
            "loss: 12.615768 [4018176/6056873]\n",
            "loss: 12.543201 [4100096/6056873]\n",
            "loss: 12.583110 [4182016/6056873]\n",
            "loss: 12.593096 [4263936/6056873]\n",
            "loss: 12.448143 [4345856/6056873]\n",
            "loss: 12.472370 [4427776/6056873]\n",
            "loss: 12.590652 [4509696/6056873]\n",
            "loss: 12.562492 [4591616/6056873]\n",
            "loss: 12.560936 [4673536/6056873]\n",
            "loss: 12.520919 [4755456/6056873]\n",
            "loss: 12.435234 [4837376/6056873]\n",
            "loss: 12.540661 [4919296/6056873]\n",
            "loss: 12.585562 [5001216/6056873]\n",
            "loss: 12.519066 [5083136/6056873]\n",
            "loss: 12.504013 [5165056/6056873]\n",
            "loss: 12.537918 [5246976/6056873]\n",
            "loss: 12.580926 [5328896/6056873]\n",
            "loss: 12.527345 [5410816/6056873]\n",
            "loss: 12.400607 [5492736/6056873]\n",
            "loss: 12.487099 [5574656/6056873]\n",
            "loss: 12.551128 [5656576/6056873]\n",
            "loss: 12.431720 [5738496/6056873]\n",
            "loss: 12.589095 [5820416/6056873]\n",
            "loss: 12.410723 [5902336/6056873]\n",
            "loss: 12.535488 [5984256/6056873]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
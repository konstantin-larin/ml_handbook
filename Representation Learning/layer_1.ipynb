{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54ca4afc",
      "metadata": {
        "id": "54ca4afc"
      },
      "source": [
        "Add to my cbow (from Neural Networks for Sequence Modeling/layer_1 ) most_similar method based on cosine similarity and make this code more flexible\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8beccc0f",
      "metadata": {
        "id": "8beccc0f"
      },
      "source": [
        "### Import libs and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf33779",
      "metadata": {
        "id": "6bf33779"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6863ccf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6863ccf3",
        "outputId": "8f9acf05-22f7-41fd-bcdc-9b07c0dcd1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('imdb')\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091e2900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "091e2900",
        "outputId": "68702eba-88c4-4c45-e0a4-b811c06d39e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12500 12500]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKLhJREFUeJzt3Xt0lOWBx/FfLkwSKTPhsrnMGgG15SKUaw3h1rXkEEtKzRZXkBTYNpJak66QrlwKBuoNjIBcpGTRKvRsKEiPsDTQSBoWcoTIJZAVA0RdULDsBD2QDETJhbz7R09eHQlKcJIwj9/POe855n2feed5H9H5njczQ5BlWZYAAAAME9zeEwAAAGgNRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAI4W29wTaU2Njo86ePatOnTopKCiovacDAACug2VZunjxotxut4KDr32/5hsdOWfPnlVcXFx7TwMAANyAM2fO6NZbb73m8W905HTq1EnS3xfJ6XS282wAAMD18Hq9iouLs1/Hr+UbHTlNv6JyOp1EDgAAAear3mrCG48BAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGCk0PaegKl6zNne3lNosfcXJ7f3FAAA18DrSstxJwcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYqcWRU1xcrPHjx8vtdisoKEhbt261j9XX12v27Nnq37+/OnbsKLfbralTp+rs2bM+5zh//rxSU1PldDoVGRmptLQ0Xbp0yWfMW2+9pVGjRik8PFxxcXHKycm5ai6bN29W7969FR4erv79+2vHjh0tvRwAAGCoFkdOTU2NBgwYoNWrV1917JNPPtHhw4f1+OOP6/Dhw3rttddUUVGhH//4xz7jUlNTVV5ersLCQuXn56u4uFjp6en2ca/Xq7Fjx6p79+4qLS3Vc889p4ULF2rt2rX2mH379unBBx9UWlqajhw5opSUFKWkpOjtt99u6SUBAAADBVmWZd3wg4OCtGXLFqWkpFxzzMGDB3X33Xfrgw8+0G233abjx4+rb9++OnjwoIYOHSpJKigo0Lhx4/Thhx/K7XZrzZo1mjdvnjwejxwOhyRpzpw52rp1q06cOCFJmjhxompqapSfn28/17BhwzRw4EDl5uZe1/y9Xq9cLpeqq6vldDpvcBWax98xAgDwJ15XPnO9r9+t/p6c6upqBQUFKTIyUpJUUlKiyMhIO3AkKTExUcHBwdq/f789ZvTo0XbgSFJSUpIqKip04cIFe0xiYqLPcyUlJamkpOSac6mtrZXX6/XZAACAmVo1ci5fvqzZs2frwQcftEvL4/EoKirKZ1xoaKi6dOkij8djj4mOjvYZ0/TzV41pOt6cRYsWyeVy2VtcXNzXu0AAAHDTarXIqa+v1wMPPCDLsrRmzZrWepoWmTt3rqqrq+3tzJkz7T0lAADQSkJb46RNgfPBBx9o165dPr8vi4mJ0blz53zGNzQ06Pz584qJibHHVFZW+oxp+vmrxjQdb05YWJjCwsJu/MIAAEDA8PudnKbAeffdd/XXv/5VXbt29TmekJCgqqoqlZaW2vt27dqlxsZGxcfH22OKi4tVX19vjyksLFSvXr3UuXNne0xRUZHPuQsLC5WQkODvSwIAAAGoxZFz6dIllZWVqaysTJJ06tQplZWV6fTp06qvr9f999+vQ4cOKS8vT1euXJHH45HH41FdXZ0kqU+fPrr33ns1ffp0HThwQHv37lVmZqYmTZokt9stSZo8ebIcDofS0tJUXl6uTZs2acWKFcrKyrLn8eijj6qgoEBLly7ViRMntHDhQh06dEiZmZl+WBYAABDoWhw5hw4d0qBBgzRo0CBJUlZWlgYNGqTs7Gz97W9/07Zt2/Thhx9q4MCBio2Ntbd9+/bZ58jLy1Pv3r01ZswYjRs3TiNHjvT5DhyXy6WdO3fq1KlTGjJkiH79618rOzvb57t0hg8frg0bNmjt2rUaMGCA/vSnP2nr1q3q16/f11kPAABgiK/1PTmBju/J8cX35ADAzYvXlc/cNN+TAwAA0B6IHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGCkFkdOcXGxxo8fL7fbraCgIG3dutXnuGVZys7OVmxsrCIiIpSYmKh3333XZ8z58+eVmpoqp9OpyMhIpaWl6dKlSz5j3nrrLY0aNUrh4eGKi4tTTk7OVXPZvHmzevfurfDwcPXv3187duxo6eUAAABDtThyampqNGDAAK1evbrZ4zk5OVq5cqVyc3O1f/9+dezYUUlJSbp8+bI9JjU1VeXl5SosLFR+fr6Ki4uVnp5uH/d6vRo7dqy6d++u0tJSPffcc1q4cKHWrl1rj9m3b58efPBBpaWl6ciRI0pJSVFKSorefvvtll4SAAAwUJBlWdYNPzgoSFu2bFFKSoqkv9/Fcbvd+vWvf61///d/lyRVV1crOjpa69at06RJk3T8+HH17dtXBw8e1NChQyVJBQUFGjdunD788EO53W6tWbNG8+bNk8fjkcPhkCTNmTNHW7du1YkTJyRJEydOVE1NjfLz8+35DBs2TAMHDlRubu51zd/r9crlcqm6ulpOp/NGl6FZPeZs9+v52sL7i5PbewoAgGvgdeUz1/v67df35Jw6dUoej0eJiYn2PpfLpfj4eJWUlEiSSkpKFBkZaQeOJCUmJio4OFj79++3x4wePdoOHElKSkpSRUWFLly4YI/5/PM0jWl6nubU1tbK6/X6bAAAwEx+jRyPxyNJio6O9tkfHR1tH/N4PIqKivI5Hhoaqi5duviMae4cn3+Oa41pOt6cRYsWyeVy2VtcXFxLLxEAAASIb9Snq+bOnavq6mp7O3PmTHtPCQAAtBK/Rk5MTIwkqbKy0md/ZWWlfSwmJkbnzp3zOd7Q0KDz58/7jGnuHJ9/jmuNaTrenLCwMDmdTp8NAACYya+R07NnT8XExKioqMje5/V6tX//fiUkJEiSEhISVFVVpdLSUnvMrl271NjYqPj4eHtMcXGx6uvr7TGFhYXq1auXOnfubI/5/PM0jWl6HgAA8M3W4si5dOmSysrKVFZWJunvbzYuKyvT6dOnFRQUpBkzZuipp57Stm3bdPToUU2dOlVut9v+BFafPn107733avr06Tpw4ID27t2rzMxMTZo0SW63W5I0efJkORwOpaWlqby8XJs2bdKKFSuUlZVlz+PRRx9VQUGBli5dqhMnTmjhwoU6dOiQMjMzv/6qAACAgBfa0gccOnRI99xzj/1zU3hMmzZN69at06xZs1RTU6P09HRVVVVp5MiRKigoUHh4uP2YvLw8ZWZmasyYMQoODtaECRO0cuVK+7jL5dLOnTuVkZGhIUOGqFu3bsrOzvb5Lp3hw4drw4YNmj9/vn7zm9/o29/+trZu3ap+/frd0EIAAACzfK3vyQl0fE+OL74nBwBuXryufKZdvicHAADgZkHkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIzk98i5cuWKHn/8cfXs2VMRERG644479OSTT8qyLHuMZVnKzs5WbGysIiIilJiYqHfffdfnPOfPn1dqaqqcTqciIyOVlpamS5cu+Yx56623NGrUKIWHhysuLk45OTn+vhwAABCg/B45zz77rNasWaMXXnhBx48f17PPPqucnBytWrXKHpOTk6OVK1cqNzdX+/fvV8eOHZWUlKTLly/bY1JTU1VeXq7CwkLl5+eruLhY6enp9nGv16uxY8eqe/fuKi0t1XPPPaeFCxdq7dq1/r4kAAAQgEL9fcJ9+/bpvvvuU3JysiSpR48e+uMf/6gDBw5I+vtdnOXLl2v+/Pm67777JEl/+MMfFB0dra1bt2rSpEk6fvy4CgoKdPDgQQ0dOlSStGrVKo0bN05LliyR2+1WXl6e6urq9PLLL8vhcOiuu+5SWVmZli1b5hNDAADgm8nvd3KGDx+uoqIivfPOO5Kk//mf/9Ebb7yhH/7wh5KkU6dOyePxKDEx0X6My+VSfHy8SkpKJEklJSWKjIy0A0eSEhMTFRwcrP3799tjRo8eLYfDYY9JSkpSRUWFLly40Ozcamtr5fV6fTYAAGAmv9/JmTNnjrxer3r37q2QkBBduXJFTz/9tFJTUyVJHo9HkhQdHe3zuOjoaPuYx+NRVFSU70RDQ9WlSxefMT179rzqHE3HOnfufNXcFi1apN/+9rd+uEoAAHCz8/udnFdffVV5eXnasGGDDh8+rPXr12vJkiVav369v5+qxebOnavq6mp7O3PmTHtPCQAAtBK/38l57LHHNGfOHE2aNEmS1L9/f33wwQdatGiRpk2bppiYGElSZWWlYmNj7cdVVlZq4MCBkqSYmBidO3fO57wNDQ06f/68/fiYmBhVVlb6jGn6uWnMF4WFhSksLOzrXyQAALjp+f1OzieffKLgYN/ThoSEqLGxUZLUs2dPxcTEqKioyD7u9Xq1f/9+JSQkSJISEhJUVVWl0tJSe8yuXbvU2Nio+Ph4e0xxcbHq6+vtMYWFherVq1ezv6oCAADfLH6PnPHjx+vpp5/W9u3b9f7772vLli1atmyZ/vmf/1mSFBQUpBkzZuipp57Stm3bdPToUU2dOlVut1spKSmSpD59+ujee+/V9OnTdeDAAe3du1eZmZmaNGmS3G63JGny5MlyOBxKS0tTeXm5Nm3apBUrVigrK8vflwQAAAKQ339dtWrVKj3++ON65JFHdO7cObndbv3iF79Qdna2PWbWrFmqqalRenq6qqqqNHLkSBUUFCg8PNwek5eXp8zMTI0ZM0bBwcGaMGGCVq5caR93uVzauXOnMjIyNGTIEHXr1k3Z2dl8fBwAAEiSgqzPfxXxN4zX65XL5VJ1dbWcTqdfz91jzna/nq8tvL84ub2nAAC4Bl5XPnO9r9/83VUAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASK0SOX/729/005/+VF27dlVERIT69++vQ4cO2ccty1J2drZiY2MVERGhxMREvfvuuz7nOH/+vFJTU+V0OhUZGam0tDRdunTJZ8xbb72lUaNGKTw8XHFxccrJyWmNywEAAAHI75Fz4cIFjRgxQh06dNBf/vIXHTt2TEuXLlXnzp3tMTk5OVq5cqVyc3O1f/9+dezYUUlJSbp8+bI9JjU1VeXl5SosLFR+fr6Ki4uVnp5uH/d6vRo7dqy6d++u0tJSPffcc1q4cKHWrl3r70sCAAABKNTfJ3z22WcVFxenV155xd7Xs2dP+58ty9Ly5cs1f/583XfffZKkP/zhD4qOjtbWrVs1adIkHT9+XAUFBTp48KCGDh0qSVq1apXGjRunJUuWyO12Ky8vT3V1dXr55ZflcDh01113qaysTMuWLfOJIQAA8M3k9zs527Zt09ChQ/Uv//IvioqK0qBBg/Tiiy/ax0+dOiWPx6PExER7n8vlUnx8vEpKSiRJJSUlioyMtANHkhITExUcHKz9+/fbY0aPHi2Hw2GPSUpKUkVFhS5cuNDs3Gpra+X1en02AABgJr9HzsmTJ7VmzRp9+9vf1uuvv65f/vKX+rd/+zetX79ekuTxeCRJ0dHRPo+Ljo62j3k8HkVFRfkcDw0NVZcuXXzGNHeOzz/HFy1atEgul8ve4uLivubVAgCAm5XfI6exsVGDBw/WM888o0GDBik9PV3Tp09Xbm6uv5+qxebOnavq6mp7O3PmTHtPCQAAtBK/R05sbKz69u3rs69Pnz46ffq0JCkmJkaSVFlZ6TOmsrLSPhYTE6Nz5875HG9oaND58+d9xjR3js8/xxeFhYXJ6XT6bAAAwEx+j5wRI0aooqLCZ98777yj7t27S/r7m5BjYmJUVFRkH/d6vdq/f78SEhIkSQkJCaqqqlJpaak9ZteuXWpsbFR8fLw9pri4WPX19faYwsJC9erVy+eTXAAA4JvJ75Ezc+ZMvfnmm3rmmWf03nvvacOGDVq7dq0yMjIkSUFBQZoxY4aeeuopbdu2TUePHtXUqVPldruVkpIi6e93fu69915Nnz5dBw4c0N69e5WZmalJkybJ7XZLkiZPniyHw6G0tDSVl5dr06ZNWrFihbKysvx9SQAAIAD5/SPk3/ve97RlyxbNnTtXTzzxhHr27Knly5crNTXVHjNr1izV1NQoPT1dVVVVGjlypAoKChQeHm6PycvLU2ZmpsaMGaPg4GBNmDBBK1eutI+7XC7t3LlTGRkZGjJkiLp166bs7Gw+Pg4AACRJQZZlWe09ifbi9XrlcrlUXV3t9/fn9Jiz3a/nawvvL05u7ykAAK6B15XPXO/rN393FQAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADBSq0fO4sWLFRQUpBkzZtj7Ll++rIyMDHXt2lXf+ta3NGHCBFVWVvo87vTp00pOTtYtt9yiqKgoPfbYY2poaPAZs3v3bg0ePFhhYWG68847tW7duta+HAAAECBaNXIOHjyo//iP/9B3v/tdn/0zZ87Un//8Z23evFl79uzR2bNn9ZOf/MQ+fuXKFSUnJ6uurk779u3T+vXrtW7dOmVnZ9tjTp06peTkZN1zzz0qKyvTjBkz9NBDD+n1119vzUsCAAABotUi59KlS0pNTdWLL76ozp072/urq6v1+9//XsuWLdMPfvADDRkyRK+88or27dunN998U5K0c+dOHTt2TP/5n/+pgQMH6oc//KGefPJJrV69WnV1dZKk3Nxc9ezZU0uXLlWfPn2UmZmp+++/X88//3xrXRIAAAggrRY5GRkZSk5OVmJios/+0tJS1dfX++zv3bu3brvtNpWUlEiSSkpK1L9/f0VHR9tjkpKS5PV6VV5ebo/54rmTkpLscwAAgG+20NY46caNG3X48GEdPHjwqmMej0cOh0ORkZE++6Ojo+XxeOwxnw+cpuNNx75sjNfr1aeffqqIiIirnru2tla1tbX2z16vt+UXBwAAAoLf7+ScOXNGjz76qPLy8hQeHu7v038tixYtksvlsre4uLj2nhIAAGglfo+c0tJSnTt3ToMHD1ZoaKhCQ0O1Z88erVy5UqGhoYqOjlZdXZ2qqqp8HldZWamYmBhJUkxMzFWftmr6+avGOJ3OZu/iSNLcuXNVXV1tb2fOnPHHJQMAgJuQ3yNnzJgxOnr0qMrKyuxt6NChSk1Ntf+5Q4cOKioqsh9TUVGh06dPKyEhQZKUkJCgo0eP6ty5c/aYwsJCOZ1O9e3b1x7z+XM0jWk6R3PCwsLkdDp9NgAAYCa/vyenU6dO6tevn8++jh07qmvXrvb+tLQ0ZWVlqUuXLnI6nfrVr36lhIQEDRs2TJI0duxY9e3bV1OmTFFOTo48Ho/mz5+vjIwMhYWFSZIefvhhvfDCC5o1a5Z+/vOfa9euXXr11Ve1fft2f18SAAAIQK3yxuOv8vzzzys4OFgTJkxQbW2tkpKS9Lvf/c4+HhISovz8fP3yl79UQkKCOnbsqGnTpumJJ56wx/Ts2VPbt2/XzJkztWLFCt1666166aWXlJSU1B6XBAAAbjJBlmVZ7T2J9uL1euVyuVRdXe33X131mBN4d5TeX5zc3lMAAFwDryufud7Xb/7uKgAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABjJ75GzaNEife9731OnTp0UFRWllJQUVVRU+Iy5fPmyMjIy1LVrV33rW9/ShAkTVFlZ6TPm9OnTSk5O1i233KKoqCg99thjamho8Bmze/duDR48WGFhYbrzzju1bt06f18OAAAIUH6PnD179igjI0NvvvmmCgsLVV9fr7Fjx6qmpsYeM3PmTP35z3/W5s2btWfPHp09e1Y/+clP7ONXrlxRcnKy6urqtG/fPq1fv17r1q1Tdna2PebUqVNKTk7WPffco7KyMs2YMUMPPfSQXn/9dX9fEgAACEBBlmVZrfkEH330kaKiorRnzx6NHj1a1dXV+od/+Adt2LBB999/vyTpxIkT6tOnj0pKSjRs2DD95S9/0Y9+9COdPXtW0dHRkqTc3FzNnj1bH330kRwOh2bPnq3t27fr7bfftp9r0qRJqqqqUkFBwXXNzev1yuVyqbq6Wk6n06/X3WPOdr+ery28vzi5vacAALgGXlc+c72v363+npzq6mpJUpcuXSRJpaWlqq+vV2Jioj2md+/euu2221RSUiJJKikpUf/+/e3AkaSkpCR5vV6Vl5fbYz5/jqYxTedoTm1trbxer88GAADM1KqR09jYqBkzZmjEiBHq16+fJMnj8cjhcCgyMtJnbHR0tDwejz3m84HTdLzp2JeN8Xq9+vTTT5udz6JFi+RyuewtLi7ua18jAAC4ObVq5GRkZOjtt9/Wxo0bW/NprtvcuXNVXV1tb2fOnGnvKQEAgFYS2lonzszMVH5+voqLi3Xrrbfa+2NiYlRXV6eqqiqfuzmVlZWKiYmxxxw4cMDnfE2fvvr8mC9+IquyslJOp1MRERHNziksLExhYWFf+9oAAMDNz+93cizLUmZmprZs2aJdu3apZ8+ePseHDBmiDh06qKioyN5XUVGh06dPKyEhQZKUkJCgo0eP6ty5c/aYwsJCOZ1O9e3b1x7z+XM0jWk6BwAA+Gbz+52cjIwMbdiwQf/1X/+lTp062e+hcblcioiIkMvlUlpamrKystSlSxc5nU796le/UkJCgoYNGyZJGjt2rPr27aspU6YoJydHHo9H8+fPV0ZGhn0n5uGHH9YLL7ygWbNm6ec//7l27dqlV199Vdu3B967zwEAgP/5/U7OmjVrVF1drX/6p39SbGysvW3atMke8/zzz+tHP/qRJkyYoNGjRysmJkavvfaafTwkJET5+fkKCQlRQkKCfvrTn2rq1Kl64okn7DE9e/bU9u3bVVhYqAEDBmjp0qV66aWXlJSU5O9LAgAAAajVvyfnZsb35Pjie3IA4ObF68pnbprvyQEAAGgPRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASAEfOatXr1aPHj0UHh6u+Ph4HThwoL2nBAAAbgIBHTmbNm1SVlaWFixYoMOHD2vAgAFKSkrSuXPn2ntqAACgnQV05CxbtkzTp0/Xz372M/Xt21e5ubm65ZZb9PLLL7f31AAAQDsLbe8J3Ki6ujqVlpZq7ty59r7g4GAlJiaqpKSk2cfU1taqtrbW/rm6ulqS5PV6/T6/xtpP/H7O1tYa6wAA8A9eV64+r2VZXzouYCPn448/1pUrVxQdHe2zPzo6WidOnGj2MYsWLdJvf/vbq/bHxcW1yhwDjWt5e88AAGCS1n5duXjxolwu1zWPB2zk3Ii5c+cqKyvL/rmxsVHnz59X165dFRQU5Lfn8Xq9iouL05kzZ+R0Ov12XvhindsOa902WOe2wTq3jdZcZ8uydPHiRbnd7i8dF7CR061bN4WEhKiystJnf2VlpWJiYpp9TFhYmMLCwnz2RUZGttYU5XQ6+Q+oDbDObYe1bhusc9tgndtGa63zl93BaRKwbzx2OBwaMmSIioqK7H2NjY0qKipSQkJCO84MAADcDAL2To4kZWVladq0aRo6dKjuvvtuLV++XDU1NfrZz37W3lMDAADtLKAjZ+LEifroo4+UnZ0tj8ejgQMHqqCg4Ko3I7e1sLAwLViw4KpfjcG/WOe2w1q3Dda5bbDObeNmWOcg66s+fwUAABCAAvY9OQAAAF+GyAEAAEYicgAAgJGIHAAAYCQi5watXr1aPXr0UHh4uOLj43XgwIEvHb9582b17t1b4eHh6t+/v3bs2NFGMw1sLVnnF198UaNGjVLnzp3VuXNnJSYmfuW/F/xdS/88N9m4caOCgoKUkpLSuhM0SEvXuqqqShkZGYqNjVVYWJi+853v8P+P69DSdV6+fLl69eqliIgIxcXFaebMmbp8+XIbzTYwFRcXa/z48XK73QoKCtLWrVu/8jG7d+/W4MGDFRYWpjvvvFPr1q1r3UlaaLGNGzdaDofDevnll63y8nJr+vTpVmRkpFVZWdns+L1791ohISFWTk6OdezYMWv+/PlWhw4drKNHj7bxzANLS9d58uTJ1urVq60jR45Yx48ft/71X//Vcrlc1ocfftjGMw8sLV3nJqdOnbL+8R//0Ro1apR13333tc1kA1xL17q2ttYaOnSoNW7cOOuNN96wTp06Ze3evdsqKytr45kHlpauc15enhUWFmbl5eVZp06dsl5//XUrNjbWmjlzZhvPPLDs2LHDmjdvnvXaa69ZkqwtW7Z86fiTJ09at9xyi5WVlWUdO3bMWrVqlRUSEmIVFBS02hyJnBtw9913WxkZGfbPV65csdxut7Vo0aJmxz/wwANWcnKyz774+HjrF7/4RavOM9C1dJ2/qKGhwerUqZO1fv361pqiEW5knRsaGqzhw4dbL730kjVt2jQi5zq1dK3XrFlj3X777VZdXV1bTdEILV3njIwM6wc/+IHPvqysLGvEiBGtOk+TXE/kzJo1y7rrrrt89k2cONFKSkpqtXnx66oWqqurU2lpqRITE+19wcHBSkxMVElJSbOPKSkp8RkvSUlJSdccjxtb5y/65JNPVF9fry5durTWNAPeja7zE088oaioKKWlpbXFNI1wI2u9bds2JSQkKCMjQ9HR0erXr5+eeeYZXblypa2mHXBuZJ2HDx+u0tJS+1daJ0+e1I4dOzRu3Lg2mfM3RXu8Fgb0Nx63h48//lhXrly56luVo6OjdeLEiWYf4/F4mh3v8XhabZ6B7kbW+Ytmz54tt9t91X9U+MyNrPMbb7yh3//+9yorK2uDGZrjRtb65MmT2rVrl1JTU7Vjxw699957euSRR1RfX68FCxa0xbQDzo2s8+TJk/Xxxx9r5MiRsixLDQ0Nevjhh/Wb3/ymLab8jXGt10Kv16tPP/1UERERfn9O7uTASIsXL9bGjRu1ZcsWhYeHt/d0jHHx4kVNmTJFL774orp169be0zFeY2OjoqKitHbtWg0ZMkQTJ07UvHnzlJub295TM8ru3bv1zDPP6He/+50OHz6s1157Tdu3b9eTTz7Z3lPD18SdnBbq1q2bQkJCVFlZ6bO/srJSMTExzT4mJiamReNxY+vcZMmSJVq8eLH++te/6rvf/W5rTjPgtXSd//d//1fvv/++xo8fb+9rbGyUJIWGhqqiokJ33HFH6046QN3In+nY2Fh16NBBISEh9r4+ffrI4/Gorq5ODoejVecciG5knR9//HFNmTJFDz30kCSpf//+qqmpUXp6uubNm6fgYO4H+MO1XgudTmer3MWRuJPTYg6HQ0OGDFFRUZG9r7GxUUVFRUpISGj2MQkJCT7jJamwsPCa43Fj6yxJOTk5evLJJ1VQUKChQ4e2xVQDWkvXuXfv3jp69KjKysrs7cc//rHuuecelZWVKS4uri2nH1Bu5M/0iBEj9N5779khKUnvvPOOYmNjCZxruJF1/uSTT64KmaawtPjrHf2mXV4LW+0tzQbbuHGjFRYWZq1bt846duyYlZ6ebkVGRloej8eyLMuaMmWKNWfOHHv83r17rdDQUGvJkiXW8ePHrQULFvAR8uvQ0nVevHix5XA4rD/96U/W//3f/9nbxYsX2+sSAkJL1/mL+HTV9WvpWp8+fdrq1KmTlZmZaVVUVFj5+flWVFSU9dRTT7XXJQSElq7zggULrE6dOll//OMfrZMnT1o7d+607rjjDuuBBx5or0sICBcvXrSOHDliHTlyxJJkLVu2zDpy5Ij1wQcfWJZlWXPmzLGmTJlij2/6CPljjz1mHT9+3Fq9ejUfIb9ZrVq1yrrtttssh8Nh3X333dabb75pH/v+979vTZs2zWf8q6++an3nO9+xHA6Hddddd1nbt29v4xkHppasc/fu3S1JV20LFixo+4kHmJb+ef48IqdlWrrW+/bts+Lj462wsDDr9ttvt55++mmroaGhjWcdeFqyzvX19dbChQutO+64wwoPD7fi4uKsRx55xLpw4ULbTzyA/Pd//3ez/89tWttp06ZZ3//+9696zMCBAy2Hw2Hdfvvt1iuvvNKqcwyyLO7FAQAA8/CeHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJH+HzhpVzg9PHNMAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "labels, counts = np.unique(train_data['label'], return_counts=True)\n",
        "print(counts)\n",
        "plt.hist(train_data['label'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KQNn4x8hkNkR",
      "metadata": {
        "id": "KQNn4x8hkNkR"
      },
      "source": [
        "#### constants for cbow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "960786ef",
      "metadata": {
        "id": "960786ef"
      },
      "source": [
        "### Preprocessing components\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1jpCW4uhTYi",
      "metadata": {
        "id": "B1jpCW4uhTYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b64ec24-1c20-4c65-8cf1-524dddbcff63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "def tokenize(text):\n",
        "    return [\n",
        "        token for token in word_tokenize(text.lower())\n",
        "        if token.isalpha() and token not in english_stopwords\n",
        "    ]\n",
        "\n",
        "class Vocabulary:\n",
        "  def __init__(self, texts=None, min_freq=5, unk_token='<UNK>', pad_token='<PAD>'):\n",
        "    self.unk_token = unk_token\n",
        "    self.pad_token = pad_token\n",
        "    self.min_freq = min_freq\n",
        "    self._word2idx = {}\n",
        "    self._idx2word = {}\n",
        "    self.tokens_counts = Counter()\n",
        "    if texts is not None:\n",
        "      self.build_vocabulary(texts)\n",
        "  def build_vocabulary(self, texts):\n",
        "    for text in texts:\n",
        "      tokens = tokenize(text)\n",
        "      self.tokens_counts.update(tokens)\n",
        "\n",
        "    self.add_token(self.unk_token)\n",
        "    self.add_token(self.pad_token)\n",
        "\n",
        "    for token, count in self.tokens_counts.items():\n",
        "      if count >= self.min_freq:\n",
        "        self.add_token(token)\n",
        "\n",
        "  def add_token(self, token):\n",
        "    if token not in self._word2idx:\n",
        "      idx = len(self)\n",
        "      self._word2idx[token] = idx\n",
        "      self._idx2word[idx] = token\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._word2idx)\n",
        "  def get_word_by_idx(self, idx):\n",
        "    if idx in self._idx2word:\n",
        "      return self._idx2word[idx]\n",
        "    return self.unk_token\n",
        "  def get_idx_by_word(self, word):\n",
        "    if word in self._word2idx:\n",
        "      return self._word2idx[word]\n",
        "    return self._word2idx[self.unk_token]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CBOWDataset(Dataset):\n",
        "  def __init__(self, texts, vocab, window_size=2):\n",
        "    self.texts = texts\n",
        "    self.vocab = vocab\n",
        "    self.window_size = window_size\n",
        "    self.pairs = self._generate_cbow_pairs()\n",
        "  def _generate_cbow_pairs(self):\n",
        "    pairs = []\n",
        "    for text in self.texts:\n",
        "      tokens = tokenize(text)\n",
        "      indexed_tokens = [self.vocab.get_idx_by_word(token) for token in tokens]\n",
        "      unk_token_idx = self.vocab.get_idx_by_word(self.vocab.unk_token)\n",
        "      pad_token_idx = self.vocab.get_idx_by_word(self.vocab.pad_token)\n",
        "      n = len(indexed_tokens)\n",
        "      for i in range(n):\n",
        "        target_word_idx = indexed_tokens[i]\n",
        "        if target_word_idx == unk_token_idx or target_word_idx == pad_token_idx:\n",
        "          # ignore unk_token as central words - it's counterproductive cause we shouldn't say cbow \"you are not right, you've predicted not unk_token\"\n",
        "          continue\n",
        "        context_indices = []\n",
        "        for j in range(max(0, i - self.window_size), i):\n",
        "          context_indices.append(indexed_tokens[j])\n",
        "        for j in range(i + 1, min(n, i + 1 + self.window_size)):\n",
        "          context_indices.append(indexed_tokens[j])\n",
        "\n",
        "        if context_indices:\n",
        "          pairs.append((context_indices, target_word_idx))\n",
        "\n",
        "\n",
        "    return pairs\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    context, target = self.pairs[idx]\n",
        "    return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super().__init__()\n",
        "    self.context_embeddings = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=True, mode='mean')\n",
        "    self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "  def forward(self, context_indices, offsets):\n",
        "    embedded_context = self.context_embeddings(context_indices, offsets)\n",
        "    return embedded_context @ self.target_embeddings.weight.T\n",
        "\n",
        "  def most_similar_words(self, idx, k=1):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      all_embeddings = self.target_embeddings.weight\n",
        "      embedding = all_embeddings[idx]\n",
        "\n",
        "      normalized_embedding = F.normalize(embedding, p=2, dim=0)\n",
        "      normalized_all_embeddings = F.normalize(all_embeddings, p=2, dim=1)\n",
        "\n",
        "      cosine_similarities = normalized_all_embeddings @ normalized_embedding.T\n",
        "\n",
        "      cosine_similarities[idx] = -float('inf')\n",
        "\n",
        "      topk_values, topk_indices = torch.topk(cosine_similarities, k=k, largest=True, sorted=True)\n",
        "      return {topk_indices[i].item(): round(topk_values[i].item(), 2) for i in range(k)}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cbow_collate_fn(batch):\n",
        "  context_tensors = []\n",
        "  target_tensors = []\n",
        "  for item in batch:\n",
        "    context_tensors.append(item[0])\n",
        "    target_tensors.append(item[1])\n",
        "  context_tensors_flatten = torch.cat(context_tensors)\n",
        "  offsets = [0] + [len(c) for c in context_tensors]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  return context_tensors_flatten, offsets, torch.stack(target_tensors)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ddcd5e",
      "metadata": {
        "id": "46ddcd5e"
      },
      "outputs": [],
      "source": [
        "gpu_t4 = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "UNKNOWN_TOKEN = '<UNK>'\n",
        "PADDING_TOKEN = '<PAD>'\n",
        "BATCH_SIZE = 512\n",
        "MIN_FREQUENCY = 5\n",
        "WINDOW_SIZE = 2\n",
        "EMBEDDING_DIM = 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset('imdb')\n",
        "train_data = data['train']\n",
        "test_data = data['test']\n",
        "train_texts = train_data['text']\n",
        "vocab = Vocabulary(train_texts, min_freq=MIN_FREQUENCY, pad_token=PADDING_TOKEN, unk_token=UNKNOWN_TOKEN)\n",
        "\n",
        "cbow_dataset = CBOWDataset(train_texts, vocab, window_size=2)\n",
        "cbow_dataloader = DataLoader(cbow_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=cbow_collate_fn)"
      ],
      "metadata": {
        "id": "vPGnjOz3uysE"
      },
      "id": "vPGnjOz3uysE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model = CBOW(len(vocab), embedding_dim=EMBEDDING_DIM).to(gpu_t4)\n",
        "top_10_movie_similars = cbow_model.most_similar_words(vocab.get_idx_by_word('movie'), k = 10)\n",
        "for key in top_10_movie_similars.keys():\n",
        "  print(f\"{vocab.get_word_by_idx(key)}: {top_10_movie_similars[key]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNiy50Kq7eb3",
        "outputId": "954476c3-cda1-4e2f-c2b3-259411e6f38d"
      },
      "id": "zNiy50Kq7eb3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natalia: 0.42\n",
            "caveman: 0.38\n",
            "kumar: 0.38\n",
            "modernist: 0.36\n",
            "forming: 0.35\n",
            "textured: 0.34\n",
            "inverted: 0.33\n",
            "resembles: 0.33\n",
            "fatally: 0.33\n",
            "separately: 0.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-244876777.py:119: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
            "  cosine_similarities = normalized_all_embeddings @ normalized_embedding.T\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ok-DW98EcPEZ",
      "metadata": {
        "id": "ok-DW98EcPEZ"
      },
      "source": [
        "#### Train CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FtrOUaboYrD3",
      "metadata": {
        "id": "FtrOUaboYrD3"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  batches_count = len(dataloader)\n",
        "  model.train()\n",
        "  for batch, (context_indices, offsets, target_word_idx) in enumerate(dataloader):\n",
        "    context_indices, offsets, target_word_idx = context_indices.to(gpu_t4), offsets.to(gpu_t4), target_word_idx.to(gpu_t4)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pred = model(context_indices, offsets)\n",
        "\n",
        "    loss = loss_fn(pred, target_word_idx)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch +1) % 100 == 0 or batch == batches_count -1:\n",
        "      loss, current = loss.item(), batch * BATCH_SIZE + len(context_indices)\n",
        "      print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "PNTl-I1Zgoyu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNTl-I1Zgoyu",
        "outputId": "d33104d0-517e-4fd0-8952-d4957e6b4743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 0------------------\n",
            "\n",
            "loss: 22.237307 [52714/2892132]\n",
            "loss: 21.573999 [103917/2892132]\n",
            "loss: 21.419586 [155105/2892132]\n",
            "loss: 21.339518 [206319/2892132]\n",
            "loss: 21.474401 [257499/2892132]\n",
            "loss: 20.865707 [308706/2892132]\n",
            "loss: 21.049643 [359907/2892132]\n",
            "loss: 20.650835 [411108/2892132]\n",
            "loss: 20.458881 [462304/2892132]\n",
            "loss: 21.104401 [513518/2892132]\n",
            "loss: 20.735323 [564708/2892132]\n",
            "loss: 21.401474 [615903/2892132]\n",
            "loss: 20.538464 [667107/2892132]\n",
            "loss: 20.521175 [718307/2892132]\n",
            "loss: 20.660736 [769511/2892132]\n",
            "loss: 20.447094 [820696/2892132]\n",
            "loss: 20.438040 [871906/2892132]\n",
            "loss: 20.647251 [923107/2892132]\n",
            "loss: 20.413202 [974313/2892132]\n",
            "loss: 20.336451 [1025511/2892132]\n",
            "loss: 20.797626 [1076702/2892132]\n",
            "loss: 19.906778 [1127915/2892132]\n",
            "loss: 20.123816 [1179107/2892132]\n",
            "loss: 20.568182 [1230307/2892132]\n",
            "loss: 20.298880 [1281516/2892132]\n",
            "loss: 20.775480 [1332709/2892132]\n",
            "loss: 20.081350 [1383911/2892132]\n",
            "loss: 20.224796 [1435108/2892132]\n",
            "loss: 20.680965 [1486309/2892132]\n",
            "loss: 20.489258 [1537516/2892132]\n",
            "loss: 20.837633 [1588707/2892132]\n",
            "loss: 20.246298 [1639915/2892132]\n",
            "loss: 20.370564 [1691107/2892132]\n",
            "loss: 19.642487 [1742306/2892132]\n",
            "loss: 20.471830 [1793520/2892132]\n",
            "loss: 20.331818 [1844714/2892132]\n",
            "loss: 20.096775 [1895910/2892132]\n",
            "loss: 20.221127 [1947116/2892132]\n",
            "loss: 19.907438 [1998305/2892132]\n",
            "loss: 20.230436 [2049511/2892132]\n",
            "loss: 19.935913 [2100704/2892132]\n",
            "loss: 19.712593 [2151916/2892132]\n",
            "loss: 20.192074 [2203107/2892132]\n",
            "loss: 20.066641 [2254304/2892132]\n",
            "loss: 20.543358 [2305501/2892132]\n",
            "loss: 20.174520 [2356709/2892132]\n",
            "loss: 20.403448 [2407887/2892132]\n",
            "loss: 19.906269 [2459101/2892132]\n",
            "loss: 20.005995 [2510295/2892132]\n",
            "loss: 19.809258 [2561516/2892132]\n",
            "loss: 19.971144 [2612708/2892132]\n",
            "loss: 20.028658 [2663914/2892132]\n",
            "loss: 19.788576 [2715110/2892132]\n",
            "loss: 19.792517 [2766309/2892132]\n",
            "loss: 20.118027 [2817510/2892132]\n",
            "loss: 20.184204 [2868707/2892132]\n",
            "loss: 20.384172 [2893183/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 1------------------\n",
            "\n",
            "loss: 19.575693 [52723/2892132]\n",
            "loss: 20.199921 [103911/2892132]\n",
            "loss: 19.684605 [155110/2892132]\n",
            "loss: 19.462715 [206308/2892132]\n",
            "loss: 19.742323 [257502/2892132]\n",
            "loss: 19.660061 [308700/2892132]\n",
            "loss: 19.411194 [359921/2892132]\n",
            "loss: 19.962910 [411116/2892132]\n",
            "loss: 19.193396 [462308/2892132]\n",
            "loss: 19.939613 [513505/2892132]\n",
            "loss: 19.531776 [564709/2892132]\n",
            "loss: 19.514065 [615916/2892132]\n",
            "loss: 19.340500 [667110/2892132]\n",
            "loss: 19.641855 [718306/2892132]\n",
            "loss: 19.379635 [769514/2892132]\n",
            "loss: 19.699072 [820709/2892132]\n",
            "loss: 19.505901 [871924/2892132]\n",
            "loss: 19.583733 [923105/2892132]\n",
            "loss: 19.603954 [974321/2892132]\n",
            "loss: 19.651566 [1025511/2892132]\n",
            "loss: 19.618721 [1076720/2892132]\n",
            "loss: 19.832685 [1127908/2892132]\n",
            "loss: 19.999001 [1179105/2892132]\n",
            "loss: 19.680094 [1230314/2892132]\n",
            "loss: 19.423311 [1281514/2892132]\n",
            "loss: 19.314291 [1332706/2892132]\n",
            "loss: 19.937971 [1383916/2892132]\n",
            "loss: 19.328979 [1435110/2892132]\n",
            "loss: 18.781401 [1486314/2892132]\n",
            "loss: 19.246815 [1537512/2892132]\n",
            "loss: 19.406860 [1588714/2892132]\n",
            "loss: 19.516529 [1639904/2892132]\n",
            "loss: 19.150110 [1691115/2892132]\n",
            "loss: 19.361107 [1742309/2892132]\n",
            "loss: 19.509933 [1793511/2892132]\n",
            "loss: 19.223204 [1844713/2892132]\n",
            "loss: 19.132263 [1895906/2892132]\n",
            "loss: 18.826361 [1947117/2892132]\n",
            "loss: 18.919416 [1998310/2892132]\n",
            "loss: 19.045046 [2049514/2892132]\n",
            "loss: 19.180676 [2100697/2892132]\n",
            "loss: 18.971001 [2151923/2892132]\n",
            "loss: 19.209106 [2203102/2892132]\n",
            "loss: 19.257528 [2254307/2892132]\n",
            "loss: 19.282652 [2305503/2892132]\n",
            "loss: 19.393946 [2356712/2892132]\n",
            "loss: 19.221264 [2407913/2892132]\n",
            "loss: 18.724844 [2459113/2892132]\n",
            "loss: 18.668856 [2510314/2892132]\n",
            "loss: 19.045395 [2561515/2892132]\n",
            "loss: 18.842215 [2612712/2892132]\n",
            "loss: 18.932009 [2663911/2892132]\n",
            "loss: 18.858118 [2715120/2892132]\n",
            "loss: 18.785582 [2766323/2892132]\n",
            "loss: 19.345758 [2817510/2892132]\n",
            "loss: 19.269455 [2868709/2892132]\n",
            "loss: 18.820751 [2893186/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 2------------------\n",
            "\n",
            "loss: 18.955992 [52709/2892132]\n",
            "loss: 18.918549 [103923/2892132]\n",
            "loss: 18.776291 [155117/2892132]\n",
            "loss: 18.905611 [206310/2892132]\n",
            "loss: 19.474035 [257517/2892132]\n",
            "loss: 18.978601 [308712/2892132]\n",
            "loss: 18.813007 [359916/2892132]\n",
            "loss: 19.358227 [411098/2892132]\n",
            "loss: 19.235424 [462307/2892132]\n",
            "loss: 18.659166 [513503/2892132]\n",
            "loss: 18.642538 [564715/2892132]\n",
            "loss: 18.584505 [615899/2892132]\n",
            "loss: 18.709188 [667104/2892132]\n",
            "loss: 18.881905 [718302/2892132]\n",
            "loss: 18.943592 [769511/2892132]\n",
            "loss: 18.997414 [820716/2892132]\n",
            "loss: 19.283106 [871902/2892132]\n",
            "loss: 18.568924 [923102/2892132]\n",
            "loss: 18.525560 [974307/2892132]\n",
            "loss: 18.980556 [1025514/2892132]\n",
            "loss: 18.672348 [1076715/2892132]\n",
            "loss: 18.808895 [1127898/2892132]\n",
            "loss: 18.498518 [1179111/2892132]\n",
            "loss: 18.921993 [1230318/2892132]\n",
            "loss: 18.774036 [1281505/2892132]\n",
            "loss: 18.573105 [1332711/2892132]\n",
            "loss: 18.942719 [1383911/2892132]\n",
            "loss: 18.654280 [1435104/2892132]\n",
            "loss: 18.797037 [1486307/2892132]\n",
            "loss: 18.823790 [1537507/2892132]\n",
            "loss: 18.870384 [1588705/2892132]\n",
            "loss: 18.920950 [1639907/2892132]\n",
            "loss: 18.556200 [1691125/2892132]\n",
            "loss: 18.661915 [1742309/2892132]\n",
            "loss: 18.848694 [1793502/2892132]\n",
            "loss: 18.514122 [1844712/2892132]\n",
            "loss: 18.843977 [1895909/2892132]\n",
            "loss: 18.628029 [1947105/2892132]\n",
            "loss: 18.843088 [1998315/2892132]\n",
            "loss: 18.727037 [2049509/2892132]\n",
            "loss: 18.987427 [2100712/2892132]\n",
            "loss: 18.649529 [2151921/2892132]\n",
            "loss: 18.973103 [2203113/2892132]\n",
            "loss: 18.705379 [2254313/2892132]\n",
            "loss: 18.487532 [2305514/2892132]\n",
            "loss: 18.479271 [2356707/2892132]\n",
            "loss: 18.438841 [2407919/2892132]\n",
            "loss: 18.403698 [2459120/2892132]\n",
            "loss: 18.956251 [2510310/2892132]\n",
            "loss: 18.473495 [2561514/2892132]\n",
            "loss: 18.257473 [2612709/2892132]\n",
            "loss: 18.448338 [2663920/2892132]\n",
            "loss: 18.526987 [2715115/2892132]\n",
            "loss: 18.290026 [2766316/2892132]\n",
            "loss: 18.364286 [2817518/2892132]\n",
            "loss: 18.384912 [2868715/2892132]\n",
            "loss: 18.575689 [2893182/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 3------------------\n",
            "\n",
            "loss: 18.437851 [52699/2892132]\n",
            "loss: 18.033468 [103913/2892132]\n",
            "loss: 17.982004 [155120/2892132]\n",
            "loss: 18.824478 [206307/2892132]\n",
            "loss: 18.354050 [257503/2892132]\n",
            "loss: 18.478806 [308715/2892132]\n",
            "loss: 18.281282 [359909/2892132]\n",
            "loss: 18.540045 [411110/2892132]\n",
            "loss: 18.572716 [462309/2892132]\n",
            "loss: 17.881365 [513512/2892132]\n",
            "loss: 18.146507 [564708/2892132]\n",
            "loss: 18.212383 [615913/2892132]\n",
            "loss: 18.702318 [667119/2892132]\n",
            "loss: 18.465687 [718306/2892132]\n",
            "loss: 18.552279 [769493/2892132]\n",
            "loss: 18.694214 [820708/2892132]\n",
            "loss: 18.337444 [871902/2892132]\n",
            "loss: 17.844450 [923103/2892132]\n",
            "loss: 17.793278 [974306/2892132]\n",
            "loss: 18.287540 [1025511/2892132]\n",
            "loss: 18.786795 [1076714/2892132]\n",
            "loss: 17.574694 [1127908/2892132]\n",
            "loss: 18.124825 [1179112/2892132]\n",
            "loss: 18.822418 [1230308/2892132]\n",
            "loss: 18.200237 [1281502/2892132]\n",
            "loss: 18.165350 [1332704/2892132]\n",
            "loss: 18.003580 [1383910/2892132]\n",
            "loss: 18.065420 [1435098/2892132]\n",
            "loss: 18.272970 [1486323/2892132]\n",
            "loss: 17.963951 [1537510/2892132]\n",
            "loss: 18.191223 [1588708/2892132]\n",
            "loss: 18.454739 [1639912/2892132]\n",
            "loss: 18.315792 [1691113/2892132]\n",
            "loss: 18.177853 [1742304/2892132]\n",
            "loss: 18.282368 [1793512/2892132]\n",
            "loss: 18.307152 [1844712/2892132]\n",
            "loss: 18.298454 [1895908/2892132]\n",
            "loss: 18.386398 [1947107/2892132]\n",
            "loss: 17.776794 [1998319/2892132]\n",
            "loss: 18.612509 [2049489/2892132]\n",
            "loss: 17.841364 [2100712/2892132]\n",
            "loss: 18.304428 [2151900/2892132]\n",
            "loss: 18.031359 [2203122/2892132]\n",
            "loss: 17.902073 [2254309/2892132]\n",
            "loss: 17.974104 [2305509/2892132]\n",
            "loss: 17.699459 [2356713/2892132]\n",
            "loss: 17.784561 [2407920/2892132]\n",
            "loss: 17.864779 [2459122/2892132]\n",
            "loss: 17.920166 [2510316/2892132]\n",
            "loss: 18.104729 [2561507/2892132]\n",
            "loss: 17.674637 [2612705/2892132]\n",
            "loss: 17.486559 [2663901/2892132]\n",
            "loss: 18.208714 [2715112/2892132]\n",
            "loss: 17.987600 [2766305/2892132]\n",
            "loss: 17.512604 [2817512/2892132]\n",
            "loss: 17.914587 [2868714/2892132]\n",
            "loss: 17.730539 [2893186/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 4------------------\n",
            "\n",
            "loss: 17.897606 [52714/2892132]\n",
            "loss: 18.135815 [103902/2892132]\n",
            "loss: 17.804600 [155109/2892132]\n",
            "loss: 18.265902 [206317/2892132]\n",
            "loss: 17.936989 [257521/2892132]\n",
            "loss: 17.961927 [308708/2892132]\n",
            "loss: 17.644983 [359900/2892132]\n",
            "loss: 17.885956 [411113/2892132]\n",
            "loss: 17.751242 [462301/2892132]\n",
            "loss: 18.067728 [513519/2892132]\n",
            "loss: 17.935886 [564706/2892132]\n",
            "loss: 18.006130 [615914/2892132]\n",
            "loss: 18.130295 [667116/2892132]\n",
            "loss: 17.510384 [718314/2892132]\n",
            "loss: 17.611359 [769501/2892132]\n",
            "loss: 17.710798 [820717/2892132]\n",
            "loss: 17.588812 [871912/2892132]\n",
            "loss: 17.639563 [923112/2892132]\n",
            "loss: 17.850018 [974306/2892132]\n",
            "loss: 17.581900 [1025519/2892132]\n",
            "loss: 17.324953 [1076719/2892132]\n",
            "loss: 17.689064 [1127905/2892132]\n",
            "loss: 17.305994 [1179107/2892132]\n",
            "loss: 18.012514 [1230313/2892132]\n",
            "loss: 17.848265 [1281505/2892132]\n",
            "loss: 18.034449 [1332716/2892132]\n",
            "loss: 18.061722 [1383900/2892132]\n",
            "loss: 17.446957 [1435113/2892132]\n",
            "loss: 17.674927 [1486319/2892132]\n",
            "loss: 17.755949 [1537508/2892132]\n",
            "loss: 17.453566 [1588709/2892132]\n",
            "loss: 17.268299 [1639917/2892132]\n",
            "loss: 17.303207 [1691113/2892132]\n",
            "loss: 17.470778 [1742303/2892132]\n",
            "loss: 18.292444 [1793501/2892132]\n",
            "loss: 17.776134 [1844712/2892132]\n",
            "loss: 17.493025 [1895906/2892132]\n",
            "loss: 17.354845 [1947112/2892132]\n",
            "loss: 17.794102 [1998309/2892132]\n",
            "loss: 17.452293 [2049506/2892132]\n",
            "loss: 17.676201 [2100706/2892132]\n",
            "loss: 17.299982 [2151912/2892132]\n",
            "loss: 17.686871 [2203105/2892132]\n",
            "loss: 17.923233 [2254297/2892132]\n",
            "loss: 18.130569 [2305506/2892132]\n",
            "loss: 17.829948 [2356715/2892132]\n",
            "loss: 17.809301 [2407903/2892132]\n",
            "loss: 17.717077 [2459119/2892132]\n",
            "loss: 17.309523 [2510315/2892132]\n",
            "loss: 17.896276 [2561506/2892132]\n",
            "loss: 17.856480 [2612720/2892132]\n",
            "loss: 17.358315 [2663903/2892132]\n",
            "loss: 17.669523 [2715114/2892132]\n",
            "loss: 17.471386 [2766312/2892132]\n",
            "loss: 17.487806 [2817507/2892132]\n",
            "loss: 18.067783 [2868713/2892132]\n",
            "loss: 17.489651 [2893183/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 5------------------\n",
            "\n",
            "loss: 17.485783 [52716/2892132]\n",
            "loss: 17.338051 [103896/2892132]\n",
            "loss: 17.291979 [155117/2892132]\n",
            "loss: 17.341778 [206317/2892132]\n",
            "loss: 17.721031 [257507/2892132]\n",
            "loss: 17.338074 [308707/2892132]\n",
            "loss: 17.185860 [359915/2892132]\n",
            "loss: 17.491711 [411117/2892132]\n",
            "loss: 17.509916 [462310/2892132]\n",
            "loss: 16.884918 [513517/2892132]\n",
            "loss: 17.304609 [564710/2892132]\n",
            "loss: 17.440868 [615895/2892132]\n",
            "loss: 17.303566 [667117/2892132]\n",
            "loss: 17.606236 [718308/2892132]\n",
            "loss: 17.538855 [769505/2892132]\n",
            "loss: 17.304722 [820712/2892132]\n",
            "loss: 17.404375 [871907/2892132]\n",
            "loss: 17.089523 [923115/2892132]\n",
            "loss: 17.500307 [974306/2892132]\n",
            "loss: 17.584032 [1025513/2892132]\n",
            "loss: 17.496792 [1076697/2892132]\n",
            "loss: 17.206671 [1127906/2892132]\n",
            "loss: 16.813976 [1179115/2892132]\n",
            "loss: 17.048250 [1230307/2892132]\n",
            "loss: 17.222324 [1281506/2892132]\n",
            "loss: 17.249542 [1332699/2892132]\n",
            "loss: 17.076729 [1383908/2892132]\n",
            "loss: 17.221731 [1435116/2892132]\n",
            "loss: 17.242985 [1486315/2892132]\n",
            "loss: 17.562061 [1537514/2892132]\n",
            "loss: 17.363447 [1588720/2892132]\n",
            "loss: 17.190016 [1639905/2892132]\n",
            "loss: 17.314535 [1691110/2892132]\n",
            "loss: 17.528358 [1742315/2892132]\n",
            "loss: 17.068859 [1793510/2892132]\n",
            "loss: 17.469519 [1844708/2892132]\n",
            "loss: 17.192986 [1895900/2892132]\n",
            "loss: 16.988531 [1947111/2892132]\n",
            "loss: 17.191479 [1998310/2892132]\n",
            "loss: 16.896999 [2049512/2892132]\n",
            "loss: 17.564911 [2100708/2892132]\n",
            "loss: 17.399696 [2151921/2892132]\n",
            "loss: 17.468060 [2203112/2892132]\n",
            "loss: 16.987875 [2254306/2892132]\n",
            "loss: 16.736341 [2305500/2892132]\n",
            "loss: 17.496792 [2356714/2892132]\n",
            "loss: 17.692156 [2407907/2892132]\n",
            "loss: 17.230417 [2459105/2892132]\n",
            "loss: 17.048435 [2510302/2892132]\n",
            "loss: 16.878601 [2561501/2892132]\n",
            "loss: 17.036182 [2612719/2892132]\n",
            "loss: 17.211834 [2663917/2892132]\n",
            "loss: 17.245611 [2715113/2892132]\n",
            "loss: 17.261614 [2766309/2892132]\n",
            "loss: 17.507004 [2817512/2892132]\n",
            "loss: 16.590042 [2868708/2892132]\n",
            "loss: 16.640837 [2893184/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 6------------------\n",
            "\n",
            "loss: 17.429125 [52717/2892132]\n",
            "loss: 17.210567 [103906/2892132]\n",
            "loss: 17.235212 [155110/2892132]\n",
            "loss: 16.879396 [206306/2892132]\n",
            "loss: 17.036894 [257506/2892132]\n",
            "loss: 17.193068 [308718/2892132]\n",
            "loss: 16.896320 [359920/2892132]\n",
            "loss: 17.146229 [411103/2892132]\n",
            "loss: 17.228474 [462303/2892132]\n",
            "loss: 16.957098 [513508/2892132]\n",
            "loss: 17.216040 [564697/2892132]\n",
            "loss: 16.923599 [615914/2892132]\n",
            "loss: 16.405092 [667110/2892132]\n",
            "loss: 16.433571 [718302/2892132]\n",
            "loss: 17.187622 [769512/2892132]\n",
            "loss: 17.168568 [820714/2892132]\n",
            "loss: 16.737770 [871918/2892132]\n",
            "loss: 16.853992 [923116/2892132]\n",
            "loss: 16.955673 [974300/2892132]\n",
            "loss: 16.823147 [1025504/2892132]\n",
            "loss: 17.057545 [1076710/2892132]\n",
            "loss: 17.204029 [1127897/2892132]\n",
            "loss: 17.176273 [1179110/2892132]\n",
            "loss: 16.704628 [1230322/2892132]\n",
            "loss: 16.791042 [1281503/2892132]\n",
            "loss: 16.523052 [1332718/2892132]\n",
            "loss: 17.139395 [1383914/2892132]\n",
            "loss: 16.793379 [1435117/2892132]\n",
            "loss: 16.952263 [1486310/2892132]\n",
            "loss: 17.106009 [1537510/2892132]\n",
            "loss: 17.199863 [1588712/2892132]\n",
            "loss: 17.102087 [1639909/2892132]\n",
            "loss: 16.847424 [1691115/2892132]\n",
            "loss: 17.007509 [1742312/2892132]\n",
            "loss: 17.132795 [1793517/2892132]\n",
            "loss: 17.268301 [1844703/2892132]\n",
            "loss: 16.927431 [1895912/2892132]\n",
            "loss: 16.753742 [1947112/2892132]\n",
            "loss: 16.686695 [1998317/2892132]\n",
            "loss: 16.926170 [2049516/2892132]\n",
            "loss: 16.952948 [2100726/2892132]\n",
            "loss: 17.149382 [2151903/2892132]\n",
            "loss: 16.868464 [2203113/2892132]\n",
            "loss: 17.078402 [2254318/2892132]\n",
            "loss: 16.629965 [2305510/2892132]\n",
            "loss: 16.732656 [2356721/2892132]\n",
            "loss: 16.390589 [2407919/2892132]\n",
            "loss: 16.128746 [2459110/2892132]\n",
            "loss: 17.114065 [2510301/2892132]\n",
            "loss: 16.744650 [2561512/2892132]\n",
            "loss: 16.664389 [2612699/2892132]\n",
            "loss: 16.770782 [2663903/2892132]\n",
            "loss: 16.534348 [2715113/2892132]\n",
            "loss: 16.984426 [2766312/2892132]\n",
            "loss: 16.657429 [2817521/2892132]\n",
            "loss: 16.445723 [2868721/2892132]\n",
            "loss: 16.289459 [2893175/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 7------------------\n",
            "\n",
            "loss: 16.630783 [52722/2892132]\n",
            "loss: 16.414148 [103914/2892132]\n",
            "loss: 16.691795 [155122/2892132]\n",
            "loss: 16.121620 [206304/2892132]\n",
            "loss: 16.781719 [257514/2892132]\n",
            "loss: 16.426998 [308712/2892132]\n",
            "loss: 16.389086 [359909/2892132]\n",
            "loss: 16.768013 [411118/2892132]\n",
            "loss: 16.701334 [462310/2892132]\n",
            "loss: 16.651869 [513514/2892132]\n",
            "loss: 16.389429 [564704/2892132]\n",
            "loss: 16.102415 [615917/2892132]\n",
            "loss: 16.274416 [667117/2892132]\n",
            "loss: 16.673574 [718316/2892132]\n",
            "loss: 16.586893 [769516/2892132]\n",
            "loss: 16.893034 [820708/2892132]\n",
            "loss: 16.320345 [871910/2892132]\n",
            "loss: 16.808304 [923108/2892132]\n",
            "loss: 16.231960 [974309/2892132]\n",
            "loss: 16.444815 [1025518/2892132]\n",
            "loss: 16.825558 [1076723/2892132]\n",
            "loss: 16.167458 [1127915/2892132]\n",
            "loss: 16.399658 [1179110/2892132]\n",
            "loss: 16.359056 [1230309/2892132]\n",
            "loss: 16.454365 [1281514/2892132]\n",
            "loss: 16.181414 [1332708/2892132]\n",
            "loss: 16.238657 [1383907/2892132]\n",
            "loss: 16.383560 [1435114/2892132]\n",
            "loss: 16.327551 [1486311/2892132]\n",
            "loss: 15.818429 [1537518/2892132]\n",
            "loss: 16.511179 [1588710/2892132]\n",
            "loss: 16.570400 [1639903/2892132]\n",
            "loss: 16.276690 [1691121/2892132]\n",
            "loss: 16.562851 [1742301/2892132]\n",
            "loss: 16.666861 [1793505/2892132]\n",
            "loss: 16.293585 [1844714/2892132]\n",
            "loss: 16.500908 [1895913/2892132]\n",
            "loss: 16.833836 [1947112/2892132]\n",
            "loss: 16.420147 [1998309/2892132]\n",
            "loss: 16.267147 [2049503/2892132]\n",
            "loss: 16.656332 [2100712/2892132]\n",
            "loss: 16.093267 [2151912/2892132]\n",
            "loss: 16.444407 [2203116/2892132]\n",
            "loss: 16.234428 [2254315/2892132]\n",
            "loss: 16.583727 [2305511/2892132]\n",
            "loss: 16.489756 [2356708/2892132]\n",
            "loss: 16.256208 [2407914/2892132]\n",
            "loss: 15.841156 [2459110/2892132]\n",
            "loss: 16.407291 [2510316/2892132]\n",
            "loss: 16.494461 [2561510/2892132]\n",
            "loss: 16.443453 [2612707/2892132]\n",
            "loss: 16.663322 [2663900/2892132]\n",
            "loss: 16.421099 [2715126/2892132]\n",
            "loss: 16.470476 [2766312/2892132]\n",
            "loss: 16.515085 [2817518/2892132]\n",
            "loss: 16.438992 [2868707/2892132]\n",
            "loss: 16.516241 [2893185/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 8------------------\n",
            "\n",
            "loss: 15.868848 [52719/2892132]\n",
            "loss: 16.376493 [103897/2892132]\n",
            "loss: 15.941550 [155106/2892132]\n",
            "loss: 16.334063 [206314/2892132]\n",
            "loss: 16.190308 [257503/2892132]\n",
            "loss: 16.255346 [308710/2892132]\n",
            "loss: 16.288792 [359922/2892132]\n",
            "loss: 16.248423 [411114/2892132]\n",
            "loss: 16.471365 [462316/2892132]\n",
            "loss: 16.417553 [513520/2892132]\n",
            "loss: 16.205090 [564721/2892132]\n",
            "loss: 16.314096 [615912/2892132]\n",
            "loss: 16.613636 [667107/2892132]\n",
            "loss: 16.119257 [718312/2892132]\n",
            "loss: 16.062931 [769503/2892132]\n",
            "loss: 16.290258 [820706/2892132]\n",
            "loss: 16.679234 [871911/2892132]\n",
            "loss: 16.275078 [923091/2892132]\n",
            "loss: 16.226240 [974310/2892132]\n",
            "loss: 16.465073 [1025495/2892132]\n",
            "loss: 16.194036 [1076699/2892132]\n",
            "loss: 16.284145 [1127906/2892132]\n",
            "loss: 16.267452 [1179113/2892132]\n",
            "loss: 16.459301 [1230313/2892132]\n",
            "loss: 16.282345 [1281506/2892132]\n",
            "loss: 16.505890 [1332706/2892132]\n",
            "loss: 15.598930 [1383917/2892132]\n",
            "loss: 16.161263 [1435110/2892132]\n",
            "loss: 16.093479 [1486314/2892132]\n",
            "loss: 16.097433 [1537505/2892132]\n",
            "loss: 16.370245 [1588710/2892132]\n",
            "loss: 16.363224 [1639912/2892132]\n",
            "loss: 16.367590 [1691100/2892132]\n",
            "loss: 16.180222 [1742299/2892132]\n",
            "loss: 16.269642 [1793506/2892132]\n",
            "loss: 16.004860 [1844712/2892132]\n",
            "loss: 16.189005 [1895909/2892132]\n",
            "loss: 16.181641 [1947107/2892132]\n",
            "loss: 16.236454 [1998306/2892132]\n",
            "loss: 16.001640 [2049505/2892132]\n",
            "loss: 16.292322 [2100712/2892132]\n",
            "loss: 16.036503 [2151904/2892132]\n",
            "loss: 16.352655 [2203112/2892132]\n",
            "loss: 16.012371 [2254307/2892132]\n",
            "loss: 15.728742 [2305513/2892132]\n",
            "loss: 15.990174 [2356717/2892132]\n",
            "loss: 16.200989 [2407912/2892132]\n",
            "loss: 15.991642 [2459118/2892132]\n",
            "loss: 16.031357 [2510313/2892132]\n",
            "loss: 16.010605 [2561509/2892132]\n",
            "loss: 16.290052 [2612710/2892132]\n",
            "loss: 16.110807 [2663915/2892132]\n",
            "loss: 15.858658 [2715106/2892132]\n",
            "loss: 15.681586 [2766315/2892132]\n",
            "loss: 16.171944 [2817505/2892132]\n",
            "loss: 15.722627 [2868702/2892132]\n",
            "loss: 15.955048 [2893190/2892132]\n",
            "\n",
            "\n",
            "\n",
            "---------------------EPOCH 9------------------\n",
            "\n",
            "loss: 15.731373 [52707/2892132]\n",
            "loss: 16.133104 [103908/2892132]\n",
            "loss: 16.095541 [155110/2892132]\n",
            "loss: 15.861651 [206314/2892132]\n",
            "loss: 15.515347 [257506/2892132]\n",
            "loss: 15.678135 [308703/2892132]\n",
            "loss: 16.189770 [359911/2892132]\n",
            "loss: 15.784794 [411109/2892132]\n",
            "loss: 16.098204 [462308/2892132]\n",
            "loss: 15.679214 [513511/2892132]\n",
            "loss: 15.951356 [564718/2892132]\n",
            "loss: 16.222628 [615913/2892132]\n",
            "loss: 15.815766 [667114/2892132]\n",
            "loss: 15.874510 [718311/2892132]\n",
            "loss: 15.702014 [769519/2892132]\n",
            "loss: 15.539493 [820709/2892132]\n",
            "loss: 16.112303 [871919/2892132]\n",
            "loss: 15.969203 [923117/2892132]\n",
            "loss: 15.961144 [974316/2892132]\n",
            "loss: 16.039209 [1025511/2892132]\n",
            "loss: 15.779734 [1076708/2892132]\n",
            "loss: 15.834794 [1127916/2892132]\n",
            "loss: 16.184587 [1179114/2892132]\n",
            "loss: 15.802050 [1230309/2892132]\n",
            "loss: 15.463047 [1281515/2892132]\n",
            "loss: 16.008398 [1332709/2892132]\n",
            "loss: 15.811409 [1383921/2892132]\n",
            "loss: 16.188198 [1435110/2892132]\n",
            "loss: 15.751949 [1486311/2892132]\n",
            "loss: 15.771438 [1537511/2892132]\n",
            "loss: 15.755098 [1588720/2892132]\n",
            "loss: 15.747066 [1639920/2892132]\n",
            "loss: 15.764639 [1691112/2892132]\n",
            "loss: 15.937327 [1742306/2892132]\n",
            "loss: 15.782274 [1793517/2892132]\n",
            "loss: 15.972764 [1844710/2892132]\n",
            "loss: 15.843112 [1895907/2892132]\n",
            "loss: 15.623933 [1947123/2892132]\n",
            "loss: 15.692301 [1998311/2892132]\n",
            "loss: 15.790565 [2049515/2892132]\n",
            "loss: 16.006435 [2100719/2892132]\n",
            "loss: 15.730196 [2151912/2892132]\n",
            "loss: 15.761292 [2203114/2892132]\n",
            "loss: 15.503675 [2254313/2892132]\n",
            "loss: 15.682033 [2305510/2892132]\n",
            "loss: 15.908397 [2356703/2892132]\n",
            "loss: 15.466145 [2407919/2892132]\n",
            "loss: 15.981222 [2459107/2892132]\n",
            "loss: 15.973168 [2510306/2892132]\n",
            "loss: 15.431582 [2561518/2892132]\n",
            "loss: 15.613902 [2612707/2892132]\n",
            "loss: 15.687461 [2663906/2892132]\n",
            "loss: 15.310542 [2715118/2892132]\n",
            "loss: 15.712331 [2766312/2892132]\n",
            "loss: 15.772356 [2817507/2892132]\n",
            "loss: 15.721547 [2868706/2892132]\n",
            "loss: 15.872696 [2893180/2892132]\n"
          ]
        }
      ],
      "source": [
        "cbow_loss_fn = nn.CrossEntropyLoss()\n",
        "cbow_optimizer = torch.optim.SGD(cbow_model.parameters(), lr=1e-2, momentum=0.8)\n",
        "for epoch in range(10):\n",
        "  print(f'\\n\\n\\n---------------------EPOCH {epoch}------------------\\n')\n",
        "  train_loop(cbow_dataloader, cbow_model, cbow_loss_fn, cbow_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_movie_similars = cbow_model.most_similar_words(vocab.get_idx_by_word('movie'), k = 10)\n",
        "for key in top_10_movie_similars.keys():\n",
        "  print(f\"{vocab.get_word_by_idx(key)}: {top_10_movie_similars[key]}\")"
      ],
      "metadata": {
        "id": "YxGeVnnGzaQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29699140-01a2-4c86-acf0-be1e341dd8da"
      },
      "id": "YxGeVnnGzaQD",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "film: 0.73\n",
            "br: 0.68\n",
            "really: 0.49\n",
            "one: 0.49\n",
            "think: 0.49\n",
            "like: 0.48\n",
            "story: 0.48\n",
            "first: 0.46\n",
            "would: 0.46\n",
            "could: 0.44\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzAYoNDi0-j0",
        "outputId": "43f84eb3-8610-4b9e-b831-9c0934af73c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ray[tune]\n",
            "  Downloading ray-2.49.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (3.19.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (2.2.2)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune])\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (18.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from ray[tune]) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tensorboardX>=1.9->ray[tune]) (2.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray[tune]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray[tune]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray[tune]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray[tune]) (0.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ray[tune]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ray[tune]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ray[tune]) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray[tune]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray[tune]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray[tune]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray[tune]) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema->ray[tune]) (4.15.0)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.49.1-cp312-cp312-manylinux2014_x86_64.whl (70.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX, ray\n",
            "Successfully installed ray-2.49.1 tensorboardX-2.6.4\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.4.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.27.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.12/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.14.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.10.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.11.1)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.12/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.9.0.20250822)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ray[tune]\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PnUMhDor0zVj"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from ray import tune\n",
        "from ray import train\n",
        "from ray.train import Checkpoint, get_checkpoint\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import ray.cloudpickle as pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "32AR59sM055o"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "    )\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    return trainset, testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ksWiTfQQ08dP"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "38zbz2Fd1UEG"
      },
      "outputs": [],
      "source": [
        "def train_cifar(config, data_dir=None):\n",
        "    net = Net(config[\"l1\"], config[\"l2\"])\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "    checkpoint = get_checkpoint()\n",
        "    if checkpoint:\n",
        "        with checkpoint.as_directory() as checkpoint_dir:\n",
        "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "            with open(data_path, \"rb\") as fp:\n",
        "                checkpoint_state = pickle.load(fp)\n",
        "            start_epoch = checkpoint_state[\"epoch\"]\n",
        "            net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
        "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs]\n",
        "    )\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\n",
        "                    \"[%d, %5d] loss: %.3f\"\n",
        "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
        "                )\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        checkpoint_data = {\n",
        "            \"epoch\": epoch,\n",
        "            \"net_state_dict\": net.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        }\n",
        "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
        "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "            with open(data_path, \"wb\") as fp:\n",
        "                pickle.dump(checkpoint_data, fp)\n",
        "\n",
        "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
        "            train.report(\n",
        "                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
        "                checkpoint=checkpoint,\n",
        "            )\n",
        "\n",
        "    print(\"Finished Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W5RRnFek2k4N"
      },
      "outputs": [],
      "source": [
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=4, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vvQ_ezC2vj-",
        "outputId": "bf3b7d71-47c1-4207-93af-3c357c676e00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.6MB/s]\n",
            "2025-09-08 19:45:42,815\tINFO worker.py:1951 -- Started a local Ray instance.\n",
            "2025-09-08 19:45:50,543\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_cifar_2025-09-08_19-45-50   |\n",
            "+--------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator             |\n",
            "| Scheduler                        AsyncHyperBandScheduler           |\n",
            "| Number of trials                 10                                |\n",
            "+--------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_cifar_2025-09-08_19-45-50\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-09-08_19-45-38_674511_315/artifacts/2025-09-08_19-45-50/train_cifar_2025-09-08_19-45-50/driver_artifacts`\n",
            "\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2025-09-08 19:45:51. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size |\n",
            "+-------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   PENDING       1      8   0.0622516                8 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2 |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4 |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16 |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2 |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8 |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2 |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16 |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16 |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16 |\n",
            "+-------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                     8 |\n",
            "| l1                                             1 |\n",
            "| l2                                             8 |\n",
            "| lr                                       0.06225 |\n",
            "+--------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=3189)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m [1,  2000] loss: 2.321\n",
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:46:21. Total running time: 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size |\n",
            "+-------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2 |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4 |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16 |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2 |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8 |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2 |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16 |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16 |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16 |\n",
            "+-------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [1,  4000] loss: 1.160\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 1 at 2025-09-08 19:46:39. Total running time: 49s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000000 |\n",
            "| time_this_iter_s                                  40.42486 |\n",
            "| time_total_s                                      40.42486 |\n",
            "| training_iteration                                       1 |\n",
            "| accuracy                                            0.0993 |\n",
            "| loss                                     2.315640926361084 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:46:51. Total running time: 1min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        1            40.4249   2.31564       0.0993 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [2,  2000] loss: 2.321\n",
            "\u001b[36m(func pid=3189)\u001b[0m [2,  4000] loss: 1.160\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 2 at 2025-09-08 19:47:17. Total running time: 1min 26s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000001 |\n",
            "| time_this_iter_s                                  37.37498 |\n",
            "| time_total_s                                      77.79984 |\n",
            "| training_iteration                                       2 |\n",
            "| accuracy                                            0.0993 |\n",
            "| loss                                      2.32987642288208 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000001)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:47:21. Total running time: 1min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        2            77.7998   2.32988       0.0993 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [3,  2000] loss: 2.320\n",
            "\u001b[36m(func pid=3189)\u001b[0m [3,  4000] loss: 1.160\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:47:51. Total running time: 2min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        2            77.7998   2.32988       0.0993 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 3 at 2025-09-08 19:47:53. Total running time: 2min 2s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000002 |\n",
            "| time_this_iter_s                                  35.92499 |\n",
            "| time_total_s                                     113.72482 |\n",
            "| training_iteration                                       3 |\n",
            "| accuracy                                            0.0969 |\n",
            "| loss                                     2.321326494216919 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000002)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m [4,  2000] loss: 2.320\n",
            "\u001b[36m(func pid=3189)\u001b[0m [4,  4000] loss: 1.160\n",
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:48:21. Total running time: 2min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        3            113.725   2.32133       0.0969 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 4 at 2025-09-08 19:48:30. Total running time: 2min 39s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000003 |\n",
            "| time_this_iter_s                                   37.32102 |\n",
            "| time_total_s                                      151.04585 |\n",
            "| training_iteration                                        4 |\n",
            "| accuracy                                             0.0969 |\n",
            "| loss                                     2.3110969066619873 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000003)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m [5,  2000] loss: 2.319\n",
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:48:51. Total running time: 3min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)     loss     accuracy |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        4            151.046   2.3111       0.0969 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                   |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                   |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                   |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                   |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                   |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                   |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                   |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                   |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                   |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [5,  4000] loss: 1.159\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 5 at 2025-09-08 19:49:07. Total running time: 3min 16s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000004 |\n",
            "| time_this_iter_s                                   37.17461 |\n",
            "| time_total_s                                      188.22046 |\n",
            "| training_iteration                                        5 |\n",
            "| accuracy                                              0.104 |\n",
            "| loss                                     2.3244779109954834 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000004)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m [6,  2000] loss: 2.321\n",
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:49:21. Total running time: 3min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        5             188.22   2.32448        0.104 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [6,  4000] loss: 1.161\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 6 at 2025-09-08 19:49:43. Total running time: 3min 52s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000005 |\n",
            "| time_this_iter_s                                   35.64872 |\n",
            "| time_total_s                                      223.86917 |\n",
            "| training_iteration                                        6 |\n",
            "| accuracy                                             0.0969 |\n",
            "| loss                                     2.3233258724212646 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 6 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000005)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:49:51. Total running time: 4min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        6            223.869   2.32333       0.0969 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [7,  2000] loss: 2.321\n",
            "\u001b[36m(func pid=3189)\u001b[0m [7,  4000] loss: 1.161\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 7 at 2025-09-08 19:50:20. Total running time: 4min 29s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000006 |\n",
            "| time_this_iter_s                                   36.93845 |\n",
            "| time_total_s                                      260.80763 |\n",
            "| training_iteration                                        7 |\n",
            "| accuracy                                             0.0993 |\n",
            "| loss                                     2.3399264812469482 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 7 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000006)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:50:21. Total running time: 4min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        7            260.808   2.33993       0.0993 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [8,  2000] loss: 2.321\n",
            "\u001b[36m(func pid=3189)\u001b[0m [8,  4000] loss: 1.160\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:50:51. Total running time: 5min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        7            260.808   2.33993       0.0993 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 8 at 2025-09-08 19:50:57. Total running time: 5min 7s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000007 |\n",
            "| time_this_iter_s                                   37.55971 |\n",
            "| time_total_s                                      298.36734 |\n",
            "| training_iteration                                        8 |\n",
            "| accuracy                                             0.0993 |\n",
            "| loss                                     2.3080461025238037 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 8 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000007)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m [9,  2000] loss: 2.320\n",
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:51:21. Total running time: 5min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        8            298.367   2.30805       0.0993 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [9,  4000] loss: 1.159\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 9 at 2025-09-08 19:51:33. Total running time: 5min 43s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000008 |\n",
            "| time_this_iter_s                                   36.12209 |\n",
            "| time_total_s                                      334.48943 |\n",
            "| training_iteration                                        9 |\n",
            "| accuracy                                             0.0995 |\n",
            "| loss                                     2.3255908489227295 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 9 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000008)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m [10,  2000] loss: 2.320\n",
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2025-09-08 19:51:51. Total running time: 6min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   RUNNING       1      8   0.0622516                8        9            334.489   2.32559       0.0995 |\n",
            "| train_cifar_6c5b1_00001   PENDING     256     64   0.0042474                2                                                    |\n",
            "| train_cifar_6c5b1_00002   PENDING      64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING       2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING      32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING      64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING     128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING     128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING       8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING       2     64   0.00574815              16                                                    |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=3189)\u001b[0m [10,  4000] loss: 1.161\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 finished iteration 10 at 2025-09-08 19:52:10. Total running time: 6min 20s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00000 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000009 |\n",
            "| time_this_iter_s                                   36.96706 |\n",
            "| time_total_s                                      371.45648 |\n",
            "| training_iteration                                       10 |\n",
            "| accuracy                                             0.1025 |\n",
            "| loss                                     2.3244049549102783 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00000 saved a checkpoint for iteration 10 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000009\n",
            "\n",
            "Trial train_cifar_6c5b1_00000 completed after 10 iterations at 2025-09-08 19:52:10. Total running time: 6min 20s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=3189)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00000_0_batch_size=8,l1=1,l2=8,lr=0.0623_2025-09-08_19-45-50/checkpoint_000009)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Trial status: 1 TERMINATED | 9 PENDING\n",
            "Current time: 2025-09-08 19:52:22. Total running time: 6min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)     loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244       0.1025 |\n",
            "| train_cifar_6c5b1_00001   PENDING       256     64   0.0042474                2                                                   |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                   |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                   |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                   |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                   |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                   |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                   |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                   |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00001 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00001 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                     2 |\n",
            "| l1                                           256 |\n",
            "| l2                                            64 |\n",
            "| lr                                       0.00425 |\n",
            "+--------------------------------------------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m /usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=5506)\u001b[0m   warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m [1,  2000] loss: 2.125\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1,  4000] loss: 0.975\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1,  6000] loss: 0.621\n",
            "\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:52:52. Total running time: 7min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)     loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2                                                   |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244       0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                   |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                   |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                   |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                   |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                   |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                   |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                   |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1,  8000] loss: 0.474\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1, 10000] loss: 0.369\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1, 12000] loss: 0.308\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:53:22. Total running time: 7min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)     loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2                                                   |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244       0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                   |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                   |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                   |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                   |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                   |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                   |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                   |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1, 14000] loss: 0.262\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1, 16000] loss: 0.232\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1, 18000] loss: 0.206\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:53:52. Total running time: 8min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)     loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2                                                   |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244       0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                   |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                   |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                   |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                   |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                   |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                   |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                   |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [1, 20000] loss: 0.183\n",
            "\n",
            "Trial train_cifar_6c5b1_00001 finished iteration 1 at 2025-09-08 19:54:09. Total running time: 8min 18s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00001 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000000 |\n",
            "| time_this_iter_s                                  107.29184 |\n",
            "| time_total_s                                      107.29184 |\n",
            "| training_iteration                                        1 |\n",
            "| accuracy                                             0.3054 |\n",
            "| loss                                     1.8859211206436157 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00001 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000000)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m [2,  2000] loss: 1.815\n",
            "\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:54:22. Total running time: 8min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        1            107.292   1.88592       0.3054 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2,  4000] loss: 0.906\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2,  6000] loss: 0.610\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2,  8000] loss: 0.472\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:54:52. Total running time: 9min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        1            107.292   1.88592       0.3054 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2, 10000] loss: 0.371\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2, 12000] loss: 0.301\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2, 14000] loss: 0.259\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:55:22. Total running time: 9min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        1            107.292   1.88592       0.3054 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2, 16000] loss: 0.232\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2, 18000] loss: 0.207\n",
            "\u001b[36m(func pid=5506)\u001b[0m [2, 20000] loss: 0.186\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:55:52. Total running time: 10min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        1            107.292   1.88592       0.3054 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00001 finished iteration 2 at 2025-09-08 19:56:01. Total running time: 10min 11s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00001 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000001 |\n",
            "| time_this_iter_s                                  112.37595 |\n",
            "| time_total_s                                      219.66779 |\n",
            "| training_iteration                                        2 |\n",
            "| accuracy                                             0.3311 |\n",
            "| loss                                     1.8288154602050781 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00001 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000001)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m [3,  2000] loss: 1.822\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3,  4000] loss: 0.898\n",
            "\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:56:22. Total running time: 10min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        2            219.668   1.82882       0.3311 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3,  6000] loss: 0.615\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3,  8000] loss: 0.468\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:56:52. Total running time: 11min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        2            219.668   1.82882       0.3311 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3, 10000] loss: 0.367\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3, 12000] loss: 0.306\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3, 14000] loss: 0.271\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:57:22. Total running time: 11min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        2            219.668   1.82882       0.3311 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3, 16000] loss: 0.231\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3, 18000] loss: 0.204\n",
            "\u001b[36m(func pid=5506)\u001b[0m [3, 20000] loss: 0.187\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:57:52. Total running time: 12min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        2            219.668   1.82882       0.3311 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00001 finished iteration 3 at 2025-09-08 19:57:59. Total running time: 12min 8s\n",
            "+-------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00001 result                        |\n",
            "+-------------------------------------------------------------+\n",
            "| checkpoint_dir_name                       checkpoint_000002 |\n",
            "| time_this_iter_s                                   117.4566 |\n",
            "| time_total_s                                      337.12438 |\n",
            "| training_iteration                                        3 |\n",
            "| accuracy                                             0.3086 |\n",
            "| loss                                     1.8880939483642578 |\n",
            "+-------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00001 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000002)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m [4,  2000] loss: 1.871\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4,  4000] loss: 0.930\n",
            "\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:58:22. Total running time: 12min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        3            337.124   1.88809       0.3086 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4,  6000] loss: 0.628\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4,  8000] loss: 0.485\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4, 10000] loss: 0.381\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:58:52. Total running time: 13min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        3            337.124   1.88809       0.3086 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4, 12000] loss: 0.313\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4, 14000] loss: 0.272\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:59:22. Total running time: 13min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        3            337.124   1.88809       0.3086 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4, 16000] loss: 0.238\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4, 18000] loss: 0.209\n",
            "\u001b[36m(func pid=5506)\u001b[0m [4, 20000] loss: 0.190\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 19:59:52. Total running time: 14min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        3            337.124   1.88809       0.3086 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00001 finished iteration 4 at 2025-09-08 19:59:58. Total running time: 14min 8s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00001 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000003 |\n",
            "| time_this_iter_s                                 119.58253 |\n",
            "| time_total_s                                     456.70692 |\n",
            "| training_iteration                                       4 |\n",
            "| accuracy                                            0.3193 |\n",
            "| loss                                     1.893980622291565 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00001 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000003)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m [5,  2000] loss: 1.938\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5,  4000] loss: 0.936\n",
            "\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 20:00:23. Total running time: 14min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        4            456.707   1.89398       0.3193 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5,  6000] loss: 0.624\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5,  8000] loss: 0.480\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 20:00:53. Total running time: 15min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        4            456.707   1.89398       0.3193 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5, 10000] loss: 0.384\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5, 12000] loss: 0.317\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5, 14000] loss: 0.273\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 20:01:23. Total running time: 15min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        4            456.707   1.89398       0.3193 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5, 16000] loss: 0.241\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5, 18000] loss: 0.216\n",
            "\u001b[36m(func pid=5506)\u001b[0m [5, 20000] loss: 0.191\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 20:01:53. Total running time: 16min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        4            456.707   1.89398       0.3193 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244        0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                    |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                    |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                    |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                    |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                    |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                    |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                    |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                    |\n",
            "+------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_6c5b1_00001 finished iteration 5 at 2025-09-08 20:02:01. Total running time: 16min 10s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_6c5b1_00001 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000004 |\n",
            "| time_this_iter_s                                 122.39187 |\n",
            "| time_total_s                                     579.09879 |\n",
            "| training_iteration                                       5 |\n",
            "| accuracy                                            0.3039 |\n",
            "| loss                                     1.910898208618164 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_6c5b1_00001 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2025-09-08_19-45-50/train_cifar_6c5b1_00001_1_batch_size=2,l1=256,l2=64,lr=0.0042_2025-09-08_19-45-51/checkpoint_000004)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(func pid=5506)\u001b[0m [6,  2000] loss: 1.920\n",
            "\u001b[36m(func pid=5506)\u001b[0m [6,  4000] loss: 0.951\n",
            "\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 20:02:23. Total running time: 16min 32s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)     loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        5            579.099   1.9109       0.3039 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244       0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                   |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                   |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                   |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                   |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                   |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                   |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                   |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [6,  6000] loss: 0.632\n",
            "\u001b[36m(func pid=5506)\u001b[0m [6,  8000] loss: 0.507\n",
            "Trial status: 1 TERMINATED | 1 RUNNING | 8 PENDING\n",
            "Current time: 2025-09-08 20:02:53. Total running time: 17min 2s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2            lr     batch_size     iter     total time (s)     loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_6c5b1_00001   RUNNING       256     64   0.0042474                2        5            579.099   1.9109       0.3039 |\n",
            "| train_cifar_6c5b1_00000   TERMINATED      1      8   0.0622516                8       10            371.456   2.3244       0.1025 |\n",
            "| train_cifar_6c5b1_00002   PENDING        64     64   0.000123616              4                                                   |\n",
            "| train_cifar_6c5b1_00003   PENDING         2     32   0.0105631               16                                                   |\n",
            "| train_cifar_6c5b1_00004   PENDING        32     64   0.0223401                2                                                   |\n",
            "| train_cifar_6c5b1_00005   PENDING        64     64   0.0851156                8                                                   |\n",
            "| train_cifar_6c5b1_00006   PENDING       128      4   0.0997883                2                                                   |\n",
            "| train_cifar_6c5b1_00007   PENDING       128      2   0.0283277               16                                                   |\n",
            "| train_cifar_6c5b1_00008   PENDING         8      1   0.000356425             16                                                   |\n",
            "| train_cifar_6c5b1_00009   PENDING         2     64   0.00574815              16                                                   |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=5506)\u001b[0m [6, 10000] loss: 0.393\n",
            "\u001b[36m(func pid=5506)\u001b[0m [6, 12000] loss: 0.323\n",
            "\u001b[36m(func pid=5506)\u001b[0m [6, 14000] loss: 0.276\n"
          ]
        }
      ],
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data(data_dir)\n",
        "    config = {\n",
        "        \"l1\": tune.choice([2**i for i in range(9)]),\n",
        "        \"l2\": tune.choice([2**i for i in range(9)]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2,\n",
        "    )\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(f\"Best trial config: {best_trial.config}\")\n",
        "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
        "\n",
        "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\")\n",
        "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
        "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "        with open(data_path, \"rb\") as fp:\n",
        "            best_checkpoint_data = pickle.load(fp)\n",
        "\n",
        "        best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
        "        test_acc = test_accuracy(best_trained_model, device)\n",
        "        print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1O3AIdF3IJN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

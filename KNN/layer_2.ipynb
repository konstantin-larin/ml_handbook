{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5543fa9",
   "metadata": {},
   "source": [
    "Let's take KNNClassifier from layer_1 and improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6018b27",
   "metadata": {},
   "source": [
    "Add cosine distance and Jaccard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1187eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8284271247461903\n",
      "0.40000000000000013\n",
      "2.8284271247461903\n",
      "0.0 because it measures vector alignment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cosine_distance(x, y):\n",
    "    return 1 - x.dot(y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "def jaccard_distance(x: set, y: set):\n",
    "    return 1 - len(x.intersection(y)) / len((x.union(y)))\n",
    "\n",
    "def euclidian_norm(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "def manhattan_norm(x, y):\n",
    "    return np.sum(np.abs(x - y))\n",
    "\n",
    "v1 = np.array([3, 1])\n",
    "v2 = np.array([1, 3])\n",
    "print(euclidian_norm(v1, v2))\n",
    "print(cosine_distance(v1, v2))\n",
    "\n",
    "v1 = np.array([3, 3])\n",
    "v2 = np.array([1, 1])\n",
    "print(euclidian_norm(v1, v2))\n",
    "print(cosine_distance(v1, v2), 'because it measures vector alignment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97899d05",
   "metadata": {},
   "source": [
    "Implement weighted KNN (uniform kernel )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64bb8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "import heapq\n",
    "from collections import Counter\n",
    "\n",
    "class KNNClassifier(ClassifierMixin, BaseEstimator):   \n",
    "    def __init__(self, norm='euclidian', k=3,kernel_type='uniform', h=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        if norm == 'euclidian':\n",
    "            self.func_norm = euclidian_norm\n",
    "        if norm == 'manhattan':\n",
    "            self.func_norm = manhattan_norm        \n",
    "        if norm == 'cosine':\n",
    "            self.func_norm = cosine_distance\n",
    "        if norm == 'jaccard':\n",
    "            self.func_norm == jaccard_distance\n",
    "\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "        self.k = k    \n",
    "        self.h = h                    \n",
    "\n",
    "    def fit(self,X : np.ndarray, y : np.ndarray):        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Wrong data')        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def get_common_class(self, closest_classes: list):        \n",
    "        classes, count = np.unique(closest_classes, return_counts=True)        \n",
    "        return classes[np.argmax(count)]\n",
    "    \n",
    "    def _get_weight(self, distance):                      \n",
    "        if distance == 0:\n",
    "            return float('inf')      \n",
    "        if self.kernel_type == 'uniform':\n",
    "            return 1.0\n",
    "        if self.kernel_type == 'distance':\n",
    "            return 1.0 / distance\n",
    "        if self.kernel_type == 'gaussian':\n",
    "            return np.exp(-(pow(distance, 2)) / (2 * pow(self.h, 2)))\n",
    "        if self.kernel_type == 'epanechnikov':\n",
    "            dist = distance / self.h\n",
    "            if np.abs(dist) <= 1:\n",
    "                return 0.75 * (1 - pow(dist,2))\n",
    "            else:\n",
    "                return 0.0    \n",
    "\n",
    "    def get_weighted_common_class(self, closest_classes, distances):\n",
    "        if self.kernel_type is None:\n",
    "            return self.get_common_class(closest_classes)\n",
    "        \n",
    "        class_weights = Counter()\n",
    "        for i in range(len(closest_classes)):\n",
    "            current_class = closest_classes[i]\n",
    "            current_distance = distances[i]\n",
    "\n",
    "            weight = self._get_weight(current_distance)            \n",
    "            if np.isinf(weight):\n",
    "                return current_class\n",
    "            \n",
    "            class_weights[current_class] += weight\n",
    "        total_weigt_sum = sum(class_weights.values())\n",
    "        if total_weigt_sum == 0:\n",
    "            # impossible to find class correctly - return no-weighted voting\n",
    "            return self.get_common_class(closest_classes)\n",
    "        \n",
    "        return max(class_weights, key=class_weights.get)\n",
    "\n",
    "    def predict(self, U):                            \n",
    "        if self.k >= self.X.shape[0]:\n",
    "            return np.full(U.shape[0], self.get_common_class(list(self.y)), dtype=int)        \n",
    "        \n",
    "        y_pred = np.zeros(U.shape[0], dtype=np.int64)         \n",
    "\n",
    "        for u_index, u in enumerate(U):            \n",
    "            k_nearest = [( -self.func_norm(u, self.X[i]) , i) for i in range(self.k)] # - для работы min heap как max heap\n",
    "            heapq.heapify(k_nearest)\n",
    "            \n",
    "            for i in range(self.k, self.X.shape[0]):                \n",
    "                max_distance = -k_nearest[0][0]\n",
    "                distance = self.func_norm(u, self.X[i])\n",
    "                if distance < max_distance:\n",
    "                    heapq.heappushpop(k_nearest, (-distance, i))\n",
    "                                            \n",
    "            k_nearest_indices = [neighbour[1] for neighbour in k_nearest]                        \n",
    "            k_nearest_distances = [-neighbour[0] for neighbour in k_nearest]                                    \n",
    "            y_pred[u_index] = self.get_weighted_common_class(self.y[k_nearest_indices], k_nearest_distances)            \n",
    "            \n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "48aa6fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42, shuffle=True)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "my_classifier = KNNClassifier(k=100, kernel_type='epanechnikov')\n",
    "my_classifier.fit(X_train, y_train)\n",
    "print(my_classifier.score(X_test, y_test))\n",
    "\n",
    "\n",
    "my_classifier = KNNClassifier(k=100, kernel_type=None)\n",
    "my_classifier.fit(X_train, y_train)\n",
    "print(my_classifier.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d872ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
